{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_digit_labels(labels):\n",
    "    '''name:         vectorize_digit_labels\n",
    "       description:  function vectorizes the labels of digits (0-9), for example: \n",
    "                         0 == [1,0,0,0,0,0,0,0,0,0]\n",
    "                         .\n",
    "                         .\n",
    "                         .\n",
    "                         5 == [0,0,0,0,0,1,0,0,0,0]\n",
    "                         .\n",
    "                         .\n",
    "                         .\n",
    "                         9 == [0,0,0,0,0,0,0,0,0,1]\n",
    "       dependencies: none\n",
    "       inputs:       labels - N x 1 vector of digit (0-9) labels\n",
    "       outputs:      labels_vectorized - N x 10 matrix of vectorized digits\n",
    "    '''\n",
    "    \n",
    "    # get number of samples and number of unique labels\n",
    "    samples = labels.shape[0]\n",
    "    num_unique = unique(labels).shape[0]\n",
    "    \n",
    "    # create a matrix of unique vectos corresponding to unique labels\n",
    "    unique_labels_vectorized = eye(num_unique)\n",
    "    \n",
    "    # create an empty matrix to store vectorized labels\n",
    "    labels_vectorized = empty((samples, num_unique))\n",
    "    \n",
    "    # get vectorized version of labe for each sample\n",
    "    for sample_idx in range(samples):\n",
    "        \n",
    "        # using label as index, because labels are digits (0-9)\n",
    "        label_idx = labels[sample_idx].astype(int)\n",
    "        \n",
    "        # adding vectorized version to matrix of labels\n",
    "        labels_vectorized[sample_idx] = unique_labels_vectorized[label_idx,:]\n",
    "        \n",
    "    # return vectorized labels\n",
    "    return labels_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pca(data, k_features):\n",
    "    '''name:         pca\n",
    "       description:  function takes an original data set an makes the following transformations: \n",
    "                     the data is centered about the origin; the covariance is then calculated; \n",
    "                     the eigenvalues and eigenvectors of the covariance are found; \n",
    "                     the original data is the projected onto the k eigenvectors in descending order \n",
    "                     of their eigenvalues, creating a new N x K matrix of k principal components\n",
    "       dependencies: none\n",
    "       inputs:       data - is an N x K matrix with the rows representing observations and columns representing features\n",
    "                     k_features - is an integer representing the number of principal components or features to keep\n",
    "       outputs:      reduced_data - an N x k_features matrix \n",
    "    '''\n",
    "    \n",
    "    # check 0 < k_features <= number of features\n",
    "    if k_features > 0 and k_features <= data.shape[1]:\n",
    "\n",
    "        # center the data and calculate the covariance matrix (sigma)\n",
    "        sigma = corrcoef(data.T)\n",
    "\n",
    "        # get the eigenvectors of sigma\n",
    "        eigen_vecs, _, _ = linalg.svd(sigma)\n",
    "\n",
    "        # create an empty matrix to hold dimensionally reduced data\n",
    "        reduced_data = empty((data.shape[0], k_features))\n",
    "\n",
    "        # for each observation x, project x onto eigenvectors\n",
    "        for observation_idx in range(data.shape[0]):\n",
    "            reduced_data[observation_idx] = dot(eigen_vecs[:,:k_features].T, data[observation_idx,:][:,newaxis])[:,newaxis].T\n",
    "\n",
    "        # return dimensionally reduced data\n",
    "        return reduced_data\n",
    "    \n",
    "    # print error message\n",
    "    print ('ERROR: 0 < k_features < %i') % data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delta_bar_delta(gamma, nabla_E, delta, up=0.1, down=0.1, phi=0.01):\n",
    "    '''name:         delta_bar_delta\n",
    "       description:  step size (gamma) is increased whenever the algorithm proceeds down the error function\n",
    "                     step size (gamma) is decreased when the algorithm jumps over a valley of the error function\n",
    "       dependencies: none\n",
    "       inputs:       gamma - vector of step sizes\n",
    "                     nabla_E - matrix of gradient errors\n",
    "                     delta - vector of exponentially averaged partial derivative in the direction of weight\n",
    "       outputs:      gamma - vector of step sizes\n",
    "                     delta_new - updated delta\n",
    "    '''\n",
    "    \n",
    "    # caculate sum of gradient error\n",
    "    nabla_E_sum = sum(nabla_E, axis=1)[:,newaxis]\n",
    "    \n",
    "    # caculate new delta\n",
    "    delta_new = (1 - phi) * nabla_E_sum + phi * delta\n",
    "    \n",
    "    # update each gamma\n",
    "    for idx in range(gamma.shape[0]):\n",
    "        \n",
    "        # if gradient is on same side, from previous, increase the step size\n",
    "        if nabla_E_sum[idx] * delta[idx] > 0: gamma[idx] + up\n",
    "        \n",
    "        # if gradient is on opposite side, form previous, increase the step size\n",
    "        elif nabla_E_sum[idx] * delta[idx] < 0: gamma[idx] * down\n",
    "            \n",
    "        # otherwise do nothing\n",
    "        else: pass\n",
    "            \n",
    "    # return modified gamma and new delta\n",
    "    return gamma, delta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def single_hidden_train(data, labels, num_features=None, k_units=20, iterations=750, output_graph=False):\n",
    "    '''name:         single_hidden_train\n",
    "       description:  function learns the weights of a single hidden layer neural network \n",
    "                     using a back propagation algorithm defined as:\n",
    "                         1. Feed-forward computation\n",
    "                         2. Backpropagation to the output layer\n",
    "                         3. Backpropagation to the hidden layer\n",
    "                         4. Weight updates using gradient descent\n",
    "                     in addition, the function dispalys a plot of the error verses the number of\n",
    "                     iterations; at this point the user may decide to continue the training or\n",
    "                     exit the function\n",
    "       dependencies: delta_bar_delta\n",
    "       inputs:       data - an N x K matrix with the rows representing observations and columns representing features\n",
    "                     labels - matrix of vectorized labels\n",
    "                     num_features - PCA is used to reduce the number of features, by default data is not reduced\n",
    "                     k_units - number of hidden units\n",
    "                     iterations - number of times the weights are updated \n",
    "                     output_graph - allows user to perform another round of iterations based on a graph of the error\n",
    "       outputs:      W_1 - trained weights for input layer\n",
    "                     W_2 - trained weights for output of hidden layer\n",
    "                     error_rate - final error rate of training data\n",
    "    '''\n",
    "    \n",
    "    # check if dimensionality is to be reduced\n",
    "    if num_features: data = pca(data, num_features)\n",
    "    \n",
    "    # get number of samples, features, and outputs\n",
    "    x_samples = data.shape[0]\n",
    "    n_features = data.shape[1]\n",
    "    m_outputs = labels.shape[1]\n",
    "    \n",
    "    # intialize initial vectors of weights, with bias term added to last row\n",
    "    W_1_bar = random.uniform(0.01, 0.1, (n_features + 1, k_units)) # (N + 1) x K\n",
    "    W_2_bar = random.uniform(0.01, 0.1, (k_units + 1, m_outputs)) # (K + 1) x M\n",
    "    \n",
    "    # define node functions sigmoid and sigmoid_prime\n",
    "    sigmoid = lambda vec_x: 1 / (1 + exp(-vec_x))\n",
    "    sigmoid_prime = lambda vec_x: sigmoid(vec_x) * (1 - sigmoid(vec_x))\n",
    "    \n",
    "    # define error function\n",
    "    error_func = lambda vec_e: 0.5 * vec_e**2\n",
    "    \n",
    "    # define initial gradient step sizes and deltas to be used in delta_bar_delta\n",
    "    gamma_1 = random.uniform(0.01, 0.1, (k_units, 1)) # K x 1\n",
    "    bar_delta_1 = zeros((k_units, 1)) # K x 1\n",
    "    gamma_2 = random.uniform(0.01, 0.1, (m_outputs, 1)) # M x 1\n",
    "    bar_delta_2 = zeros((m_outputs, 1)) # M x 1\n",
    "    \n",
    "    # define offset and create arrays to store sample error & sum of errors\n",
    "    offset = 0\n",
    "    sample_error = zeros((x_samples,1))\n",
    "    sum_of_errors = zeros((iterations,1))\n",
    "    \n",
    "    # while user chooses to continue\n",
    "    iterate = True\n",
    "    while iterate:\n",
    "        \n",
    "        # iterate for specified number of steps\n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            # loop through each sample\n",
    "            for sample_idx in range(x_samples):\n",
    "\n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step one: feed-forward computation\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # get input vector which, for consistancy, we will call output_0 and add bias term\n",
    "                output_0 = data[sample_idx,:][newaxis,:] # 1 x N\n",
    "                output_0_hat = hstack((output_0, ones((1, 1)))) # 1 x (N + 1)\n",
    "                \n",
    "                # caculate output_1 and add bias term\n",
    "                output_1 = sigmoid(dot(output_0_hat, W_1_bar)) # 1 x K\n",
    "                output_1_hat = hstack((output_1, ones((1, 1)))) # 1 x (K + 1)\n",
    "                \n",
    "                # caculate output_2 and convert to vectorized label\n",
    "                output_2 = sigmoid(dot(output_1_hat, W_2_bar)) # 1 x M\n",
    "                new_output_2 = zeros(output_2.shape)\n",
    "                new_output_2[:,argmax(output_2)] = 1\n",
    "                \n",
    "                # caculate derivatives of output_1 and output_2 and store in diagonal matrices\n",
    "                D_1 = diagflat(sigmoid_prime(dot(output_0_hat, W_1_bar))) # K x K\n",
    "                D_2 = diagflat(sigmoid_prime(dot(output_1_hat, W_2_bar))) # M x M\n",
    "                \n",
    "                # caculate difference in error\n",
    "                error_diff = new_output_2 - labels[sample_idx,:][newaxis,:] # 1 x M\n",
    "                \n",
    "                # caculate sum of the sample's error\n",
    "                sample_error[sample_idx] = sum(error_func(error_diff))\n",
    "                \n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step two: backpropagation to the output layer\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # define backpropagated error of output layer\n",
    "                delta_2 = dot(D_2, error_diff.T) # M x 1\n",
    "                \n",
    "                # define error gradient of output layer\n",
    "                nabla_E_2 = dot(delta_2, output_1_hat) # M x (K + 1)\n",
    "                \n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step three: backpropagation to the hidden layer\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # define backpropagated error of hidden layer\n",
    "                delta_1 = dot(D_1, dot(W_2_bar[:-1,:], delta_2)) # K x 1\n",
    "                \n",
    "                # define error gradient of hidden layer\n",
    "                nabla_E_1 = dot(delta_1, output_0_hat) # K x (N + 1)\n",
    "                \n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step four: weight updates\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # get gamma_1 and gamma_2 using delta-bar-delta\n",
    "                gamma_1, bar_delta_1 = delta_bar_delta(gamma_1, nabla_E_1, bar_delta_1)\n",
    "                gamma_2, bar_delta_2 = delta_bar_delta(gamma_2, nabla_E_2, bar_delta_2)\n",
    "                \n",
    "                # update weights\n",
    "                W_1_bar += (-gamma_1 * nabla_E_1).T # (N + 1) x K\n",
    "                W_2_bar += (-gamma_2 * nabla_E_2).T # (K + 1) x M\n",
    "                \n",
    "            # add all sample errors to sum of errors\n",
    "            sum_of_errors[iteration + offset] = sum(sample_error) / x_samples\n",
    "        \n",
    "        # output a graph and let the user decide to continue or not\n",
    "        if output_graph:\n",
    "            \n",
    "            # output graph of error vs. steps\n",
    "            plot(sum_of_errors)\n",
    "            xlim([0,sum_of_errors.shape[0]-1])\n",
    "            ylim([0,sum_of_errors[argmax(sum_of_errors)]])\n",
    "            xlabel('Number of Iterations')\n",
    "            ylabel('Sum of Error (%)')\n",
    "            title_str = ('3 Layer NN w/ %i Hidden Units at %i Iterations') % (k_units, iterations)\n",
    "            title(title_str)\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(show())\n",
    "\n",
    "            # output prompt to continue or exit\n",
    "            response = raw_input('Continue training neural network ([y]/n)? ')\n",
    "            if response.lower() != 'n': \n",
    "                offset += iterations\n",
    "                sum_of_errors = vstack((sum_of_errors, zeros((iterations,1))))\n",
    "            else: iterate = False\n",
    "                \n",
    "        # else continue iterating till \n",
    "        else: iterate = False\n",
    "\n",
    "    # return weights with bias row removed and associated error rate\n",
    "    return W_1_bar[:-1,:], W_2_bar[:-1,:], (sum(sample_error) / x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_hidden_predict(data, m_outputs, weight_1, weight_2, k_units=20):\n",
    "    '''name:         single_hidden_predict\n",
    "       description:  functions uses previously trained weights to predict the output of a given data set\n",
    "       dependencies: none\n",
    "       inputs:       data - an N x K matrix with the rows representing observations and columns representing features\n",
    "                     m_outputs - number of possiable labeles\n",
    "                     weight_1 - trained weights for input layer\n",
    "                     weight_2 - trained weights for output layer\n",
    "                     k_units - number of hidden units\n",
    "       outputs:      y_hat - vector of predicted labels\n",
    "    '''\n",
    "    \n",
    "    # get number of samples\n",
    "    x_samples = data.shape[0]\n",
    "    \n",
    "    # define node function sigmoid\n",
    "    sigmoid = lambda vec_x: 1 / (1 + exp(-vec_x))\n",
    "    \n",
    "    # initialize vector for labels\n",
    "    labels = empty((x_samples,1))\n",
    "    \n",
    "    # loop through each observation\n",
    "    for sample_idx in range(x_samples):\n",
    "    \n",
    "        # get input vector, call it output zero for consistency \n",
    "        output_0 = data[sample_idx,:][newaxis,:]\n",
    "        \n",
    "        # caculate output one\n",
    "        output_1 = sigmoid(dot(output_0, weight_1))\n",
    "        \n",
    "        # caculate output two\n",
    "        output_2 = sigmoid(dot(output_1, weight_2))\n",
    "        \n",
    "        # convert output two to a single digit label and to array of labels\n",
    "        labels[sample_idx] = argmax(output_2)\n",
    "        \n",
    "    # return predicted labels\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(y, y_hat):\n",
    "    '''name:         confusion_matrix\n",
    "       description:  function displays a matrix of true positives and false positives\n",
    "       dependencies: none\n",
    "       inputs:       y - actual labels\n",
    "                     y_hat - predicted labels\n",
    "       outputs:      none\n",
    "    '''\n",
    "    \n",
    "    # convert y's as integer arrays\n",
    "    y = y.astype(int)\n",
    "    y_hat = y_hat.astype(int)\n",
    "    \n",
    "    # get the quantity of unique labels\n",
    "    num_labels = unique(y).shape[0]\n",
    "    \n",
    "    # initilize confusion matrix and label quantity vector\n",
    "    confusion_mat = zeros((num_labels, num_labels))\n",
    "    label_quantities = zeros(num_labels)\n",
    "    \n",
    "    # for each observed / predicted label, \n",
    "    for idx in range(y.shape[0]):\n",
    "        \n",
    "        # add 1 to the corresponding cell\n",
    "        confusion_mat[y[idx], y_hat[idx]] += 1\n",
    "        \n",
    "    # get quantity of each actual label\n",
    "    for label in range(num_labels):\n",
    "        label_quantities[label] = y[y[:] == label].shape[0]\n",
    "        \n",
    "    # print confusion matrix\n",
    "    print divide(confusion_mat, label_quantities[:,newaxis]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEZCAYAAAB4hzlwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xe8XFW5//HPN4USSiihGwhVLv4EAhpRWgQuhCIoipAr\n",
       "SBNBRbxXUEBAA3ql2LhIEbkoFq54UVDwUkI7dCERCC1BAoSSUAIECD3l+f2x1kx2JufMmXPOzJk5\n",
       "53zfr9e8Zpe19372lP3stXZTRGBmZgYwqNkBmJlZ63BSMDOzMicFMzMrc1IwM7MyJwUzMytzUjAz\n",
       "szInBRswJH1B0vVVxrdJOryDcaMkLZTUcv8ZSRdIOrnZcQw0kk6UdFGz46i3lvuBtxJJv5f0vKQ3\n",
       "JD0p6aQqZQ+RdHtvxlcLSTMkvShpWGHYlyTdUuhfKOlBSSoM+4GkX9cphgslHdHO8IMlTZb0uqRn\n",
       "JZ0paXBh/CqSrpT0Zl6P8VWW0e7nn6fbGSAiLo2I3aqEGvnVq/Lnv0HFsAmSflfL9BHxlYj4QZ5u\n",
       "rKRnGxRnh0kzj99e0tyK10JJn8njD5G0oGL8DoXpu/195/I71Wtd21neEp9rRJweEUv8rvs6J4Xq\n",
       "TgfWj4gVgd2Br0sa1+SY2qWsg9GDgG90Mou1gAMK/fXcOI4D/q+d4cuS4loV+BiwM3BcYfx5wLvA\n",
       "6sAXgAskbdbFZTdlQ18HrRhz1Zgi4vaIWKH0AvYC3gSuKxS7s1gmIm4rjOvJ9x1AR7//qjr57ww4\n",
       "TgpVRMQjEfFuYdB84KWuzkfSoZIezTWOJyR9uTDuYUl7FfqHSnpZ0ha5fxtJd0maI+kBSTsWyrbl\n",
       "Pfo7gbeA9dtbDeDHwHGShlcJ8yzg1OKeepX1uVXSvrl727w3uEfu31nS/YWymwOvRcSsJQKL+EVE\n",
       "3BkR8/P4S4Ft83TLAfsCp0TE2xFxJ/BX4KDO4qsSd+Xe5b9KmibpNUk/J21UlMcNlvRjSbMlPQHs\n",
       "WTGv4ZIuljRL0nOSvq/ctJSXc4ekH0l6VamW2dWdiWKtbWxexjeVan2zJB1SGH9JXv4w4Fpg7bwX\n",
       "/oakNSWNKdTIXpD0kw4+n5Uk/U3SSznuqyWtk8f9J7A9cG6e9zk1rMMhwOUR8U5761Wx7G5/37lG\n",
       "tS5wdY7tuDy8K/+dDTr6n+bYKj/XtVRRm5O0t6RH8vJukbRpYdwMScdKmpJ/b5dJWjqPG5E/9zmS\n",
       "XpF0WzOTlJNCJySdL+kt4BHgBxFxXzdm8yKwZ65xHAr8TNLoPO43wIGFsnsAMyNiSv5D/g04LSJW\n",
       "Ju1F/1nSqoXyBwJfApYHnulg+ZOBNhbfC690JfAG6Y8M1fe62oCxuXtH4Elgh0J/W8X6/K3KvIp2\n",
       "BB7O3ZsA8yNiemH8FOBDNc6rKkkjgD8D3yHVVJ4gJaTS3vARpESwJfAR4HMsvqd8CfA+sCEwGtiV\n",
       "9D2UjAGm5XmfBVzcw5DXAFYE1gYOB84rJPkAIiLeJtXKZuW98BUj4gXgv4CfRcRwYAPgfztYxqAc\n",
       "57r59Q5wLmnmJwG3A1/L8z6mWrB5Q/pZ0u+7JIDROdE+Junkwk5It7/viDiI9NvfK8f24278d56m\n",
       "g/9pRLzFkp/r8xR+D5I2Af4HOAYYAVxDSlJDCuu+H7Abaedtcxb9144Fns3TrQ6cGE28/5CTQici\n",
       "4qukH80uwA8kjenGPK6JiKdy923ARNJeF6S94z0lLZ/7DwJKex8HAtdExHV52htJG/jSXmsAl0TE\n",
       "1IhYGBHzOwoB+C6p+WtEB2UWAqcAp0ga2skq3UragJPX4/RC/455fMkepD9IVZIOA7Yi1WogfeZv\n",
       "VBSbC6xQZTbb5L2t8ou0cWvPHsDDEXFFRCyIiLOBFwrjP0/akM6MiDnAD1lUi1iD1Jz4HxHxTkTM\n",
       "Bs5m8ea3pyPi4vzn/i2wlqTVq38KVc0jbeAWRMS1pGaZDxbGq+K96H1gY0kj8l74Pe0tICJejYgr\n",
       "I+LdiHiTtM47VhSrdQ92X2B2RfPQbcCHImI1UsIYD3wrj+vO911Nl/87nfxP21vv4rD9gb9FxE0R\n",
       "sYD0O14W+EShzDkR8UL+PV1N2uGA9P2sBYzK3++d3VznunBSqEEkbcDlpB9yl0jaXdLfc9VwDmmD\n",
       "tGqe9yzgTuBzklYi7ZFcmiddD9ivYiO3LbBmYfY1HVSMiEdIe04n0EHbcN7YPAcc2VGZ7O/AJnkj\n",
       "tyVpozcy74V9lPTnJ6/PpsBd1WKT9GnSBmj3iHg1D36TtGdcNJy0oegwrohYufii49rT2qR1LSp+\n",
       "lmtV9Bfnsx4wFHi+8L38AlitUKacYPIePKQNX3sW5PkVDSUlgpJXImJhof/tKvOrdDhpT3yqpHsl\n",
       "7dleIUnDlE4KmCHpdVJyH17RlFHrHuzBpN/FogkjnoqIp3P3w8BppBoYdO/7rqbL/51q/9MarE3h\n",
       "N5J3Bp4F1imUKe50vMOi7+9HwHRgYm62Or7GZTaEk0LXDCW1P9Ystxv+mdSEsHreUF3D4nsZpSak\n",
       "/YC7ctUU0o/sdxUbuhUi4qzCtF2pZn6P1CyyTpUyJ5GaVIZ1VCBv5P4B/DvwUETMI234jwWmFzbs\n",
       "uwE3VasKK7W1/5JU9X+kMOqfwBBJGxWGbcGi5qWemgWMLMShYj/wPIvXMordzwLvAasWvpfhEfHh\n",
       "bsbyDEseD1ofmNGFeUTF+6IREdMj4t/yHvqZwJ8kLdvOPI4lJY8xualpRwrHWdqbd3skjczT/raz\n",
       "soV59/T7roytS/+dGv6nna37TFIiKs2v9Hua2Vm8EfFmRBwXERsCewPfVAPPpOqMk0IHJK0m6QBJ\n",
       "yykddNyNtNH+a/XJtLSkZUovYKn8ehlYKGl3Uvtz0ZWkppNjWPyP9HvgU5J2zTEso3TQsbhRr/mA\n",
       "VEQ8AfyRKmciRcStpD/iwVT/I9wKfI1FTUVtwNEs2XTU3llHAOQf/qXAvhExuSKOt4ArgNPyHux2\n",
       "wKdY1LTWU9cAH5L0mdzuewyL70X+L3CMpHUkrUyqYZVie57UtPBTSStIGiRpQxVOr+yiPwIn52UN\n",
       "krQL6cydP9U4fXHD/SKwqqTyXrekAyWVajGvk77XhSxpedIe7OuSViHtRBS9SDqG0pmDSGcZPbVY\n",
       "kGlPfI3cvSlwMvAXqMv3XRlbV/87nf1Pl/hcK1xOagbeKTe/Hks6k6qjWnLxRIK9JG2UE8kbpJrj\n",
       "gs5WuFGcFDoWwFGkJoZXgO8DB0XEpCrlP0H6U72dX2/l92NIG5lXSc1PiyWWSGc4XQGMyu+l4c8B\n",
       "+5D23F8i7f0cy+I/5q4ekDqNVAsoTlc5j5OBVTqZz62kjUipzfg2YDkWNR2J9Ke6rt2pFy1nBeBa\n",
       "LTpvvZhEvkpql32J9Cc/KiKmdjCvWk49LZeJiJdJSf4M0oZgI+COQtmLgOtJBzsnk/Yii/P/Imkj\n",
       "8ijpe72cRUmlvViqxXYaaeNxR57XGcC/RcSjNU5fXK9pwB+AJ5XOIFqLVGN7WNJc4GfAARHxXjvz\n",
       "OZv0eb+c47m2Yrn/RWrmfFXS2VXiOYjFDzCX7ARMkfQmaWfhz6Rmw5KefN+nkxLrHEnf7Op/JyLm\n",
       "UuV/2sHnWvzcHyPV9n8OzCYdu/hUJ8f5SsvfCLiB1FR2F3Be3jlrCjXxILcVSDoF2DgivtjsWOoh\n",
       "H5A/JyK2aXYsZla7IZ0XsUbLVfXD6ME5+C0oWLL5wcxanJuPmkzp9g/PANdGxB2dle8rImJSRHR4\n",
       "nyEza01uPjIzszLXFMzMrKxPH1OQ5GqOmVk3RES7p7P36aQAHa9Yq5I0ISImNDuOruhrMfe1eMEx\n",
       "94a+Fi80LuZqO9RuPjIzszInBTMzK3NS6H1tzQ6gG9qaHUAXtTU7gG5oa3YA3dDW7AC6qK3ZAXRD\n",
       "W28vsE+fkiop+toxBTOzZqu27XRNwczMypwUzMyszEnBzMzKnBTMzKzMScHMzMqcFMzMrMxJwczM\n",
       "ypwUzMyszEnBzMzKnBTMzKysoUlB0jhJ0yQ9Lun4dsZ/QdIUSQ9KulPS5oVxM/Lw+yXd28g4zcws\n",
       "adjzFCQNBs4FdgFmApMkXRURUwvFngR2iIjXJY0Dfglsk8cFMDYiXm1UjGZmtrhG1hTGANMjYkZE\n",
       "zAMuA/YpFoiIuyPi9dx7D/CBinn4ZndmZr2okUlhHeDZQv9zeVhHDgeuKfQHcKOkyZKOaEB8ZmZW\n",
       "oZGP46z5ntySPgkcBmxbGLxtRDwvaTXgBknTIuL2egdpZmaLNDIpzARGFvpHkmoLi8kHly8CxkXE\n",
       "nNLwiHg+v8+WdCWpOWqJpCBpQqG3LSLa6hG8mVl/IWksMLamso16yI6kIcBjwM7ALOBeYHzxQLOk\n",
       "dYGbgQMj4u+F4cOAwRExV9JywETg1IiYWLEMP2THzKyLqm07G1ZTiIj5ko4GrgcGAxdHxFRJR+bx\n",
       "FwLfBVYGLpAEMC8ixgBrAlfkYUOASysTgpmZ1Z8fx2lmNsD4cZxmZlYTJwUzMytzUjAzszInBTMz\n",
       "K3NSMDOzMicFMzMrc1IwM7MyJwUzMytzUjAzszInBTMzK3NSMDOzMicFMzMrc1IwM7MyJwUzMytz\n",
       "UjAzszInBTMzK3NSMDOzMicFMzMrc1IwM7MyJwUzMytzUjAzszInBTMzK3NSMDOzMicFMzMrc1Iw\n",
       "M7MyJwUzMytzUjAzszInBTMzK3NSMDOzMicFMzMrc1IwM7MyJwUzMytraFKQNE7SNEmPSzq+nfFf\n",
       "kDRF0oOS7pS0ea3TmplZ/SkiGjNjaTDwGLALMBOYBIyPiKmFMh8HHo2I1yWNAyZExDa1TJunj4hQ\n",
       "Q1bAzKyfqrbtbGRNYQwwPSJmRMQ84DJgn2KBiLg7Il7PvfcAH6h1WjMzq79GJoV1gGcL/c/lYR05\n",
       "HLimm9OamVkdDGngvGtul5L0SeAwYNtuTDuh0NsWEW21TmtmNhBIGguMraVsI5PCTGBkoX8kaY9/\n",
       "Mfng8kXAuIiY05VpASJiQj2CNTPrr/LOclupX9L3OirbyOajycDGkkZJWgrYH7iqWEDSusAVwIER\n",
       "Mb0r05qZWf01rKYQEfMlHQ1cDwwGLo6IqZKOzOMvBL4LrAxcIAlgXkSM6WjaRsVqZmZJw05J7Q0+\n",
       "JdXMrOuadUqqmZn1MU4KZmZW5qRgZmZlTgpmZlbmpGBmZmVOCmZmVlb1OgVJqwP7ATsAo0i3n3ga\n",
       "uA24PCJeanSAZmbWezq8TkHSxcCGwLXAvcDzgIC1SHcxHUe6k+mXeifUdmP0dQpmZl1UbdtZLSls\n",
       "HhEPdjLjTss0kpOCmVnXdSspdDCjjYBlI+KhegXXE04KZmZdV23bWfO9jySdRGpOWihp6Yg4qF4B\n",
       "mplZa+gwKUj6BnBuRCzIgzaPiP3zuKY1GZmZWeNUOyX1FeB6SXvn/hskXSfpBtLdS83MrJ+pekxB\n",
       "0rLAcaSzjU4B/gksFRGv9U541fmYgplZ13X7QLOk/wfMA94ATsuDT4mIF+oeZTc4KZiZdV23DjRL\n",
       "+g3wPjAMmBURR0gaDVwkaVJEnNbRtGZm1jdVO/toy4jYQumRaPcBRMT9wKck7dMr0ZmZWa+qlhSu\n",
       "kzQRGAr8T3FERPy1oVGZmVlTdHZMYTiwMCLm9l5ItfMxBTOzruvW4zglHQK81VFCkLSUpEPrE6KZ\n",
       "mbWCas1HywOTJE0DJgEvkG6ItybwEWBT4KKGR9gJCUVQ+706zMysQ501HwnYFtgOWDcPfhq4A7gr\n",
       "unLjpAaQFBCDI1jYzDjMzPqSut0Qr9XkpDAkggWdlzYzM+jmMQUzMxt4+kNS8NlHZmZ1UjUpSBok\n",
       "6fO9FUw3OSmYmdVJ1aQQEQuB43splu5yUjAzq5Namo9ukHScpJGSVim9Gh5Z7ZwUzMzqpNOzjyTN\n",
       "gCWuA4iI2KBRQdUqn320bATvNjsWM7O+okeP44yIUXWPqL5cUzAzq5NOk4KkpYCvADuQagy3Ar+I\n",
       "iHkNjq1WTgpmZnVSyzGFC4CtgPNy99b5vVOSxkmaJulxSUscsJa0qaS7Jb0r6diKcTMkPSjpfkn3\n",
       "VltMLbGYmVnnOq0pAB+NiM0L/TdJerCziSQNBs4FdgFmku6jdFVETC0UewX4OvDpdmYRwNiIeLWG\n",
       "GM3MrA5qqSnMl7RRqUfShsD8GqYbA0yPiBm5qekyYLGH80TE7IiYTHrkZ3tqqQW4pmBmVie11BS+\n",
       "Bdws6ancPwqo5ZbZ6wDPFvqfAz7WhdgCuFHSAuDCiOjojqxOCmZmdVI1KeQmoC2ATYAP5sGPRUQt\n",
       "p4D29E5720bE85JWI10rMS0ibm8vzB4ux8zMsqpJISIWSBofET8FpnRx3jOBkYX+kaTaQk0i4vn8\n",
       "PlvSlaTmqHaSwvATpTfeyz1tEdHWxTjNzPo1SWOBsTWVreHitZ+RntP8R+At0p55RMR9nUw3BHgM\n",
       "2BmYBdwLjK840FwqOwGYGxE/yf3DgMERMVfScsBE4NSImFgxXUCsHMFrtaysmZn18HkKktpopyko\n",
       "Ij5Zw4J3B84GBgMXR8Tpko7M018oaU3SU91WBBYCc4HNgNWBK/JshgCXRsTp7a0YxCoRzOksFjMz\n",
       "S7qdFPIxhW/k5qOWk5PCqhH4tFUzsxp1+yE7EbEAGN+QqOrHB5rNzOqkllNS75B0Ll08pmBmZn1P\n",
       "Q48pNFpuPlotgpebHYuZWV/RowPNrSwnhdUjmN3sWMzM+opuHVOQdHah+xsV4y6pW3Q952MKZmZ1\n",
       "Uu1A846F7kMqxm1R/1C6zUnBzKxOarkhXqtzUjAzq5NqZx8Nzs9iVqGbUn/DI6udk4KZWZ1USwor\n",
       "Av/I3Sp0txonBTOzOukwKfSBZzOXOCmYmdVJfzimYGZmddIfkoJrCmZmdVLtOoX1ezOQHnBSMDOr\n",
       "k2o1hT8BSLq5l2LpLicFM7M66eyU1JOATSR9k8U3vtFCt9N2UjAzq5NqNYUDgAWkaxJWyK/lC92t\n",
       "wknBzKxOarlL6h4RcU0vxdMl+YZ4G0TwVLNjMTPrK7r9kJ3sLkk/k/SP/PqJpOF1jrEnXFMwM6uT\n",
       "WpLCr4A3gP2Az5Oeo/zrRgbVRU4KZmZ1Ukvz0ZSI2KKzYc2Qm482iuCJZsdiZtZX9LT56B1J2xdm\n",
       "th3wdr2CqwPXFMzM6qSWZzQfBfy2cBxhDnBw40LqMicFM7M6qflxnKWkEBGvNzSiLsjNR/8SwbRm\n",
       "x2Jm1ldUaz6qpaYAtFYyqLBUswMwM+sv+sMN8ZZudgBmZv2Fk4KZmZV12nwkaQiwJzCqUL6V7n3k\n",
       "5iMzszqp5ZjC1cA7wEPAwsaG0y2uKZiZ1UktSWGdiNi84ZF0n5OCmVmd1HJMYaKk3RoeSfc5KZiZ\n",
       "1UktNYW7gCslDQLm5WERESs2LqwucVIwM6uTWpLCT4FtgIcjwscUzMz6sVqaj54BHulOQpA0TtI0\n",
       "SY9LOr6d8ZtKulvSu5KO7cq0BT77yMysTmqpKTwF3CLpWuD9PKzTU1IlDQbOBXYBZgKTJF0VEVML\n",
       "xV4Bvg58uhvTlrimYGZWJ7XUFJ4CbibtkXflcZxjgOkRMSMi5gGXAfsUC0TE7IiYzKJjFTVPW+Ck\n",
       "YGZWJ53WFCJiQjfnvQ7wbKH/OeBjDZjWScHMrE5quaL5lnYGR0Ts1Mmktd1+tcfTjt9ZuqzU0xYR\n",
       "bT1YrplZvyNpLDC2lrK1HFP4VqF7GeCzwPwappsJjCz0jyTt8deiC9P+4d6IP0yocb5mZgNO3llu\n",
       "K/VL+l5HZWtpPppcMegOSZNqiGMysLGkUcAsYH9gfAdlK+/r3ZVp3XxkZlYntTQfrVLoHQR8BOj0\n",
       "wrWImC/paOB6YDBwcURMlXRkHn+hpDWBSXl+CyV9A9gsIt5sb9oOFjWis1jMzKw2nT55TdIMFrXx\n",
       "zwdmAKdGxB0NjawG+clr90bUfADbzGzAq/bktZofx9mKclKYHcHqzY7FzKyvqJYUOrxOQdIYSWsV\n",
       "+g+WdJWkcyqalJpteYnlmx2EmVl/UO3itQuB9wAk7QCcAfwGeAP4ZeNDq9kM0gOAzMysh6olhUER\n",
       "8Wru3h+4MCL+HBEnAxs3PrSaPQVs0OwgzMz6g2pJYbCkobl7F6B4EVst1zf0lieB9ZsdhJlZf1Bt\n",
       "4/4H4FZJLwNvA7cDSNoYeK0XYqvVTGDtZgdhZtYfdJgUIuI/Jd0MrAlMLNw6W6Q7m7aKl4FNmh2E\n",
       "mVl/ULUZKCLubmfYPxsXTrfMBlZrdhBmZv1BLbfObnVOCmZmdeKkYGZmZU4KZmZW1h+SwuvAMpLv\n",
       "lmpm1lN9PilEEKQzkFxbMDProT6fFDI3IZmZ1YGTgpmZlfWXpPAS+PbZZmY91V+Swix8qwszsx7r\n",
       "L0nhGWDdZgdhZtbX9Zek8CwwstlBmJn1df0lKTwDrNfsIMzM+rr+khSmAxtJ/WZ9zMyaol9sRCN4\n",
       "HZgLrNPsWMzM+rJ+kRSyR4Etmx2EmVlf1p+SwpXA55sdhJlZX9afksLNwDbNDsLMrC9TRDQ7hm6T\n",
       "FBGh1M1Q4A1gRARvNTcyM7PWVdx2Vuo3NYUI5gH3AAc1OxYzs76q3ySF7Gxg72YHYWbWV/W3pPBP\n",
       "YKNmB2Fm1lf1m2MKqZ9lSE9iWz43J5mZWYUBcUwBIIJ3gaeBTZsdi5lZX9TQpCBpnKRpkh6XdHwH\n",
       "Zc7J46dIGl0YPkPSg5Lul3RvFxY7Gdi6p7GbmQ1EQxo1Y0mDgXOBXYCZwCRJV0XE1EKZPYCNImJj\n",
       "SR8DLmDRtQYBjI2IV7u46IeAzXq8AmZmA1AjawpjgOkRMSMi5gGXAftUlNkb+A1ARNwDrCRpjcL4\n",
       "dtu8OvEcvgeSmVm3NDIprEN6zkFJexvramUCuFHSZElHdGG5M/FT2MzMuqVhzUekjXotOqoNbBcR\n",
       "syStBtwgaVpE3L7ExNKEQm8bxCxcUzAzK5M0FhhbS9lGJoWZLP40tJGkmkC1Mh/Iw4iIWfl9tqQr\n",
       "Sc1RSySFiJhQ7JdYHlhbYqUIXuvhOpiZ9XkR0Qa0lfolfa+jso1sPpoMbCxplKSlgP2BqyrKXAV8\n",
       "EUDSNsBrEfGipGGSVsjDlwN2JR1A7lQEb+b5fqE+q2FmNnA0rKYQEfMlHQ1cDwwGLo6IqZKOzOMv\n",
       "jIhrJO0haTrwFnBonnxN4ApJpRgvjYiJXVj8zcD2wHl1Wh0zswGhX13RvGg4mwPXAB+OYE7vR2Zm\n",
       "1roGzBXNBQ8BD5OukTAzsxr1y6QQQQAzgBFNDsXMrE/pl0khexnYVOrWBXBmZgNSf08KxwCfbnYg\n",
       "ZmZ9RX9OCu/m9y2bGoWZWR/Sn5PC/Pw+rqlRmJn1If05KVwCjAJGSYxqaiRmZn1Ev7xOYfEyXEpK\n",
       "fgdE+KCzmVm1bedASApbk265gZOCmdkATwqpXPmOrctF8HaDwzIza2kD8YrmSqUzkXaQGnpnWDOz\n",
       "Pm2gJIV1gCeAa4F9mxyLmVnLGhBJIYJXgVVy7/oSq0n8RhoY629mVquB1JSyG+kGeWNIyfCLwG+B\n",
       "m5oZlJlZKxkwe8oRTAJ+CexMemjP08DopgZlZtZiBkxSAIjgFeBx0rNK/wxs1NSAzMxazIBKCtk7\n",
       "+f1G4IPNDMTMrNUMxKQwnXQm0h3AhyVCYq0mx2Rm1hIGYlL4MrBZBHOBz+Vhn2xiPGZmLWPAJYUI\n",
       "5kfwfu5uA44DTpXYramBmZm1gAGXFNpxJ+mA83USRzQ7GDOzZnJSgPvy+06kGsOyABI7S2zSvLDM\n",
       "zHrfgLghXu3z43LgKSCAjwPXRnB6veZvZtYKqm07B9IVzbWYADycu4N0TYOZ2YDh5qPFPZrf5wMC\n",
       "NiyNkFhd4iNS+R5KZmb9jmsKBRGExFDgPGB9YIzE3sABwPhSOYl1gXkRvNCcSM3MGsPHFNqdb/m6\n",
       "hW8D49op8iiwGTAmgkn5bquDI5hX71jMzOptwD95rfvzZwTprKTrgY8CNwC/Ag4DngHmAl8j3Utp\n",
       "qwj2aVQsZmb14qRQt+WxBjCClCBeB64gPdVtGeAV4DMR3J7LDgL2iuCq3orPzKwWPvuoTiJ4EXgR\n",
       "eERi4zx4PPB54B/AbRK7ApOAvYDfSawMfIT0HIcfl66mNjNrRa4p9Gj5DC0dR5AYzaIL4YraSM1L\n",
       "kB7qcyHwRh52fgQLGx6omVlBtW1nQ09JlTRO0jRJj0s6voMy5+TxUySN7sq0zVY8sBzB/cDmpGak\n",
       "6YViY0nNTXuSnvZ2J/AQ8HPgUYmjJdYDkBgi8XOJMbl/Kd96w8x6VUQ05AUMJm0cRwFDgQeAf6ko\n",
       "swdwTe7+GPD3WqfN5aJR8fds3UMQgyF+mEIMIAZBnA+3BMSPIfaAiPx6F2Jm7n4b4i2IkyB2zsO2\n",
       "hfg3iOMhjigs578hls3L+x3EhyEuh/hyocyqECv0bF0Y24TPcAjEsG7+9nolXojhED/N38GyPZtX\n",
       "bTFDqLe/i2Z/zgM13kbGXG3b2ciV+ThwXaH/BOCEijK/APYv9E8D1qxl2s5WrBVeeWO9SqF/TTji\n",
       "kUL/xvl9o0KCOCO/vw9xPcR0iHcK4wNim7yxD4ixELtWjH8hz+fTEO9BzIH4Vi3xVsR+WJrfXtdC\n",
       "XADxxU6m3w7ikm5+VqtAfBViH4iJhXVZtbN4c6x/gjg0rfd657TzPUyAGFHr91bjd/vtHOOF+f27\n",
       "EGt0UP4siF2r/F8mQHwGYu0qy9yltJNRGLZcjfGO6CxxdTXh5JiHV8xjQ4iPtPNZjcw7LVV/Q418\n",
       "AROatexWi7lZSeFzwEWF/gOBn1eUuRr4RKH/RmBr4LOdTdvZirXqq6MvGeJzEJtCrAbxzbyRmQKx\n",
       "IsSeEEMhjoK4K487J7//AOJuiC/l/sMqEsTfC91L52RyHMSgiuUPymV2zf37QsxIw75XSjQB8SM6\n",
       "2IOHuCqXqZz3ihB3QEzOG43vQxwEMSqPXwripjztfIgnID4OcU1etx9CfAXiEIh98zQ75fKbljaW\n",
       "i17HPANxD8T9eT0+m8cd1U7Mw/LyR0BsBbF1LrsJxIF5md/Nn8WxpFrbKBbV7O7I789BzM3dB+fX\n",
       "dhBX5+kD4jWIz+flKs9nJYgtYNn/LKxDRY06Ns6fwZQ8/rc5ltIOxB/a+S6/A7FDXs6aEAvy57ov\n",
       "xEqFsocYIHc8AAAKBklEQVTk+ZVqpXdAjM/T7VX4jr6aP5OfQHwZYjBseF6e5q95vvcV1mFy/qw2\n",
       "hLht8e8nxuV5rgWxWe5eHmJMM/57rfxqRlJo5NlHUWO5ph0obiUR/KnQ+1OJ24CZEbwB/F8e/gvg\n",
       "FxL7kp4x/UvgJOBu4Nf5FcA/SRfYvQKcCkwkPWnuBWApYBhwksSCwjJLx5eulHgnlzkMeBKuPyZi\n",
       "woESXwP2Bl6Q2j2L6n3gTeBlabED6MuSru94k9Qs+BawHIDEq6RTegeTTvcV8GoECyU+TToWczhw\n",
       "IulYzCoSF+Vp/od01pdIV51vBlwCz15EOl14Ri4zNH8OZ0n8oCLmFVl0W5NlcjfAY8DtwEqkq9uX\n",
       "z5/1kLw+JwCzSdew/BD4cgQLJLYEfp+XuW6e5wbAv+d5XChxfh4fwArAE/DtjUnf20LgAYk383gB\n",
       "q6TvgWeAfUg7XFvlec8Ctpd4ubBOg/O6H0hqgo28/nOAM4B18/zJ464FfkK6GeTHgW1JV/W/A6wk\n",
       "8W6OoVReqfz4pYHjgQ+Rfo/HAScDdwHfB/4IrJOnOzMPXxu4PN85YBBA/r0NAYZJTAGepSGO2kRi\n",
       "68bMu1F6P+aGnX0kaRtSlhuX+08EFkbEmYUyvwDaIuKy3D8N2JH0J6w6bR7emODNzPq5aMJ1CpOB\n",
       "jSWNIu3N7E/h/kHZVcDRwGU5ibwWES9KeqWGaTtcKTMz656GJYWImC/paFL1ejBwcURMlXRkHn9h\n",
       "RFwjaQ9JpSaFQ6tN26hYzcws6dMXr5mZWX312ecptOLFbZJ+JelFSQ8Vhq0i6QZJ/5Q0UdJKhXEn\n",
       "5vinSdq1STGPlHSLpEckPSzpmFaOW9Iyku6R9ICkRyWd3srxVsQ+WNL9kq7uCzFLmiHpwRzzvX0k\n",
       "5pUk/UnS1Pz7+Firxizpg/mzLb1el3RM0+Nt9ilX3TydqqaL25oQ1/bAaOChwrCzgG/n7uOBM3L3\n",
       "ZjnuoXk9pgODmhDzmsCWuXt50lk3/9LKcQPD8vsQ4O/Adq0cbyHubwKXAlf1kd/GU8AqFcNaPebf\n",
       "AIcVfh/DWz3mHMsg4HlgZLPj7fWVr9MHWNPFbU2KbRSLJ4VpwBq5e01gWu4+ETi+UO46YJsWiP8v\n",
       "wC59IW7SabOTSKdEtnS8wAdI1+F8Eri6L/w2clJYtWJYy8acE8CT7Qxv2ZgLy94VuL0V4u2rzUfr\n",
       "sPi5zM+x6HzoVrNGRLyYu18E1sjda5PiLmn6OuSzvUYD99DCcUsaJOmBHNctEfEILRxv9jPgW7DY\n",
       "9RutHnMAN0qaLKl0D65Wjnl9YLakX0u6T9JFkpajtWMuOQD4Q+5uarx9NSn0yaPjkdJ7tdibtl6S\n",
       "liddgPSNiJhbHNdqcUfEwojYkrT3vYOkT1aMb6l4Je0FvBQR99PBxZqtFnO2bUSMBnYHviZp+8UC\n",
       "ar2Yh5Au6js/IrYindF4wmIBtV7MSFoK+BRw+RLBNCHevpoUZpLa3kpGsngGbSUvSloTQNJawEt5\n",
       "eOU6fCAP63WShpISwu8i4i95cMvHHRGvk6723prWjvcTwN6SniLtDe4k6Xe0dsxExPP5fTZwJemZ\n",
       "IK0c83PAcxExKff/iZQkXmjhmCEl3X/kzxma/Bn31aRQvjAuZ9n9oWWfcHYVcHDuPpjUZl8afoCk\n",
       "pSStD2wM3NvbwUkScDHwaEScXRjVknFLGlE6G0PSssC/Ave3arwAEfGdiBgZEeuTmglujoiDWjlm\n",
       "ScMkrZC7lyO1eT/UyjFHxAvAs5I2yYN2AR4h3WOtJWPOxrOo6agUV/PibcZBlTodmNmddKbMdODE\n",
       "ZseTY/oD6Qrs90nHPA4l3TPmRtJ9bSYChZuR8Z0c/zRgtybFvB35fjukjev9wLhWjRv4MOlhRg8A\n",
       "DwLfysNbMt524t+RRWcftWzMpPb5B/Lr4dJ/rJVjzjFsQTr5YArp/lfDWzlm0j3AXgZWKAxrary+\n",
       "eM3MzMr6avORmZk1gJOCmZmVOSmYmVmZk4KZmZU5KZiZWZmTgpmZlTkpWEuRtFDSjwv9x0n6Xp3m\n",
       "fYmkz9ZjXp0sZ7982+abKoaPUr6tuqQtJO1ex2UOl/SVQv/akpa4bYJZZ5wUrNW8D3xG0qq5v54X\n",
       "0nR7XpK68pTCw4EvRcTOVcqMBvaoYwwrA18t9UTErIjYryvzNwMnBWs984BfAv9ROaJyT1/Sm/l9\n",
       "rKRbJf1F0hOSzpB0kKR780NiNijMZhdJkyQ9JmnPPP1gST/K5adI+nJhvrdL+ivpdgmV8YzP839I\n",
       "0hl52HeBbYFfSTqrvRXM95o6Ddg/P1xlP0nLKT2k6Z58h8+9c9lDJF2Vax035HI3SvpHXvbeebZn\n",
       "ABvm+Z0paT1JD+d5LJPvHPpgnvfYwryvkHSt0gNdzix8Hpfk9XpQ0r/X9tVZf9CwZzSb9cD5wIPt\n",
       "bFQr9/SL/ZsDmwJzSM8BuCgixig9Se7rpCQjYL2I+KikjYBb8vvBwGu5/NLAHZIm5vmOBj4UEU8X\n",
       "FyxpbdKGeCvgNWCipH0i4jSlu7YeGxH3tbdyETFP0inA1hFRetLdD4GbIuKwfG+neyTdWIjhwxHx\n",
       "mqTBwGciYq6kEcDdpHviHJ/jHJ3nN6rw+XwNWBARm0v6YI61dH+gLYAtSTW0xyT9nHSr5rUj4sN5\n",
       "XsPbWw/rn1xTsJYT6dbdvwWO6cJkkyLixYh4n3RvmOvz8IdJDz6CtJH837yM6cCTpESyK/BFSfeT\n",
       "nuS2CrBRnubeyoSQfZT0LIdXImIB6YlqOxTGt3uL7IrxxTK7AifkGG4BlgbWzTHfEBGv5XKDgNMl\n",
       "TQFuANaWtHony9sW+H1e78eAp4FN8rxvioi5EfEe8Ghe5hPABpLOkbQb8EYn62L9iGsK1qrOJt34\n",
       "7teFYfPJOzKSBgFLFca9V+heWOhfSPXfeWlv+uiIuKE4IjezvFVluuKGWCxec+nO8Yt9I+Lxihg+\n",
       "VhHDF4ARwFYRsUDpdtzL1DDvjpJG8XNbAAzJNZItgN2Ao4DPk46T2ADgmoK1pIiYQ9qrP5xFG9gZ\n",
       "pGcnAOxNelZtVwjYT8mGwAaku01eD3y1dCBX0iaShnUyr0nAjpJWzU06BwC3diGWN4AVCv3XU6gZ\n",
       "SRpdiLloRdIDexbkZqr18vC5FfMrup2UTMjNRuuS1ru9RKF8kH9wRFwBnEJqIrMBwknBWk1xD/sn\n",
       "pL3ikotIG+IHgG2ANzuYrnJ+Ueh+hnQP+muAI3Nz03+Tmk7uy6eMXkCqXXT41KtID6A5gdTU8wAw\n",
       "OSKu7sL63QJsVjrQDHwfGJoP7D4MnNpO/JCaqT4i6UHgIGBqjucV4M58cPjMiunOBwblaS4DDo6I\n",
       "eR2sX5Ae8XhLbsr6HRVPL7P+zbfONjOzMtcUzMyszEnBzMzKnBTMzKzMScHMzMqcFMzMrMxJwczM\n",
       "ypwUzMyszEnBzMzK/j+ZSlnANfYeIQAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bf3dd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue training neural network ([y]/n)? n\n"
     ]
    }
   ],
   "source": [
    "# get the training data from file\n",
    "with gzip.open('../Data/zip.train.gz') as f:\n",
    "    train = loadtxt(f)\n",
    "    \n",
    "# separate the observations from the labels and vectorize the labels\n",
    "X_train = train[:,1:]\n",
    "y_train = vectorize_digit_labels(train[:,0][:,newaxis])\n",
    "\n",
    "# train weights and get error rate\n",
    "W_1, W_2, error_rate = single_hidden_train(X_train, y_train, num_features=X_train.shape[1]-1, output_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82  0.03  0.04  0.    0.    0.08  0.01  0.    0.02  0.  ]\n",
      " [ 0.    0.95  0.01  0.    0.    0.02  0.    0.02  0.    0.  ]\n",
      " [ 0.08  0.06  0.73  0.01  0.03  0.04  0.01  0.05  0.01  0.02]\n",
      " [ 0.13  0.09  0.14  0.27  0.    0.34  0.02  0.02  0.01  0.  ]\n",
      " [ 0.04  0.07  0.02  0.    0.02  0.24  0.01  0.16  0.08  0.36]\n",
      " [ 0.05  0.26  0.08  0.01  0.    0.44  0.09  0.03  0.03  0.  ]\n",
      " [ 0.11  0.03  0.28  0.    0.    0.04  0.54  0.01  0.01  0.01]\n",
      " [ 0.    0.01  0.01  0.    0.    0.03  0.    0.91  0.02  0.01]\n",
      " [ 0.01  0.06  0.16  0.    0.01  0.27  0.04  0.02  0.39  0.04]\n",
      " [ 0.01  0.05  0.01  0.02  0.    0.22  0.    0.14  0.19  0.37]]\n"
     ]
    }
   ],
   "source": [
    "# get the test data \n",
    "with gzip.open('../Data/zip.test.gz') as f:\n",
    "    test = loadtxt(f)\n",
    "    \n",
    "# separate the observations from the labels\n",
    "X_test = test[:,1:]\n",
    "y_test = test[:,0][:,newaxis]\n",
    "\n",
    "# reduce the number of features to optimal dimensions\n",
    "X_test = pca(X_test, X_test.shape[1]-1)\n",
    "\n",
    "# make predictions \n",
    "y_hat = single_hidden_predict(X_test, unique(y_test).shape[0], W_1, W_2)\n",
    "\n",
    "# output confusion matrix\n",
    "confusion_matrix(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
