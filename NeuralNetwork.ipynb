{
 "metadata": {
  "name": "",
  "signature": "sha256:acdaf779a8dc165d446f6c527c8fcf15dc5fa0bb5de91afc9517dbd5cb1b065a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import numpy as np\n",
      "from IPython import display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_digit_labels(labels):\n",
      "    '''name:         vectorize_digit_labels\n",
      "       description:  function vectorizes the labels of digits (0-9), for example: \n",
      "                         0 == [1,0,0,0,0,0,0,0,0,0]\n",
      "                         .\n",
      "                         .\n",
      "                         .\n",
      "                         5 == [0,0,0,0,0,1,0,0,0,0]\n",
      "                         .\n",
      "                         .\n",
      "                         .\n",
      "                         9 == [0,0,0,0,0,0,0,0,0,1]\n",
      "       dependencies: none\n",
      "       inputs:       labels - N x 1 vector of digit (0-9) labels\n",
      "       outputs:      labels_vectorized - N x 10 matrix of vectorized digits\n",
      "                     num_unique - number of unique labels\n",
      "    '''\n",
      "    \n",
      "    # get number of samples and number of unique labels\n",
      "    samples = labels.shape[0]\n",
      "    num_unique = np.unique(labels).shape[0]\n",
      "    \n",
      "    # create a matrix of unique vectos corresponding to unique labels\n",
      "    unique_labels_vectorized = np.eye(num_unique)\n",
      "    \n",
      "    # create an empty matrix to store vectorized labels\n",
      "    labels_vectorized = np.empty((samples, num_unique))\n",
      "    \n",
      "    # get vectorized version of labe for each sample\n",
      "    for sample_idx in range(samples):\n",
      "        \n",
      "        # using label as index, because labels are digits (0-9)\n",
      "        label_idx = labels[sample_idx].astype(int)\n",
      "        \n",
      "        # adding vectorized version to matrix of labels\n",
      "        labels_vectorized[sample_idx] = unique_labels_vectorized[label_idx,:]\n",
      "        \n",
      "    # return vectorized labels\n",
      "    return labels_vectorized, num_unique"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pca(data, k_features):\n",
      "    '''name:         pca\n",
      "       description:  function takes an original data set an makes the following transformations: \n",
      "                     the data is centered about the origin; the covariance is then calculated; \n",
      "                     the eigenvalues and eigenvectors of the covariance are found; \n",
      "                     the original data is the projected onto the k eigenvectors in descending order \n",
      "                     of their eigenvalues, creating a new N x K matrix of k principal components\n",
      "       dependencies: none\n",
      "       inputs:       data - is an N x K matrix with the rows representing observations and columns representing features\n",
      "                     k_features - is an integer representing the number of principal components or features to keep\n",
      "       outputs:      reduced_data - an N x k_features matrix \n",
      "    '''\n",
      "    \n",
      "    # check 0 < k_features <= number of features\n",
      "    if k_features > 0 and k_features <= data.shape[1]:\n",
      "\n",
      "        # center the data and calculate the covariance matrix (sigma)\n",
      "        sigma = corrcoef(data.T)\n",
      "\n",
      "        # get the eigenvectors of sigma\n",
      "        eigen_vecs, _, _ = np.linalg.svd(sigma)\n",
      "\n",
      "        # create an empty matrix to hold dimensionally reduced data\n",
      "        reduced_data = np.empty((data.shape[0], k_features))\n",
      "\n",
      "        # for each observation x, project x onto eigenvectors\n",
      "        for observation_idx in range(data.shape[0]):\n",
      "            reduced_data[observation_idx] = np.dot(eigen_vecs[:,:k_features].T, data[observation_idx,:][:,np.newaxis])[:,np.newaxis].T\n",
      "\n",
      "        # return dimensionally reduced data\n",
      "        return reduced_data\n",
      "    \n",
      "    # print error message\n",
      "    print ('ERROR: 0 < k_features < %i') % data.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def delta_bar_delta(gamma, nabla_E, delta, up=0.5, down=0.5, phi=0.5):\n",
      "    '''name:         delta_bar_delta\n",
      "       description:  step size (gamma) is increased whenever the algorithm proceeds down the error function\n",
      "                     step size (gamma) is decreased when the algorithm jumps over a valley of the error function\n",
      "       dependencies: none\n",
      "       inputs:       gamma - vector of step sizes\n",
      "                     nabla_E - matrix of gradient errors\n",
      "                     delta - vector of exponentially averaged partial derivative in the direction of weight\n",
      "       outputs:      gamma - vector of step sizes\n",
      "                     delta_new - updated delta\n",
      "    '''\n",
      "    \n",
      "    # caculate sum of gradient error\n",
      "    nabla_E_sum = np.sum(nabla_E, axis=1)\n",
      "    \n",
      "    # caculate new delta\n",
      "    delta_new = (1 - phi) * nabla_E_sum + phi * delta\n",
      "\n",
      "    # update each gamma\n",
      "    for idx in range(gamma.shape[0]):\n",
      "        \n",
      "        # if gradient is on same side, from previous, increase the step size\n",
      "        if nabla_E_sum[idx] * delta[idx] > 0: gamma[idx] + up\n",
      "        \n",
      "        # if gradient is on opposite side, form previous, increase the step size\n",
      "        elif nabla_E_sum[idx] * delta[idx] < 0: gamma[idx] * down\n",
      "            \n",
      "        # otherwise do nothing\n",
      "        else: pass\n",
      "            \n",
      "    # return modified gamma and new delta\n",
      "    return gamma, delta_new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def single_hidden_train(data, labels, m_output, k_hidden=10, iterations=100):\n",
      "    '''name:         single_hidden_train\n",
      "       description:  function learns the weights of a single hidden layer neural network \n",
      "                     using a back propagation algorithm defined as:\n",
      "                         1. Feed-forward computation\n",
      "                         2. Backpropagation to the output layer\n",
      "                         3. Backpropagation to the hidden layer\n",
      "                         4. Weight updates using gradient descent\n",
      "                     in addition, the function dispalys a plot of the error verses the number of\n",
      "                     iterations; at this point the user may decide to continue the training or\n",
      "                     exit the function\n",
      "       dependencies: delta_bar_delta\n",
      "       inputs:       data - is an N x K matrix with the rows representing observations and columns representing features\n",
      "                     labels - matrix of vectorized labels\n",
      "                     k_units - number of hidden units\n",
      "                     m_output - number of output labels\n",
      "                     iterations - number of times the weights are updated \n",
      "       outputs:      W_1 - trained weights for input layer\n",
      "                     W_2 - trained weights for output of hidden layer\n",
      "    '''\n",
      "    \n",
      "    # get number of samples and features\n",
      "    x_samples = data.shape[0]\n",
      "    n_features = data.shape[1]\n",
      "    \n",
      "    # intialize initial vectors of weights, with bias term\n",
      "    W_1 = np.random.random((n_features, k_hidden)) # N x K\n",
      "    W_1_bar = np.vstack((W_1, np.ones((1, k_hidden)))) # (N + 1) x K\n",
      "    W_2 = np.random.random((k_hidden, m_output)) # K x M\n",
      "    W_2_bar = np.vstack((W_2, np.ones((1, m_output)))) # (K + 1) x M\n",
      "    \n",
      "    # define unit functions sigmoid and sigmoid_prime\n",
      "    sigmoid = lambda o, W: 1 / (1 + np.exp(np.dot(o,W)))\n",
      "    sigmoid_prime = lambda s: s * (1 - s)\n",
      "    \n",
      "    # define output error function error and error_prime\n",
      "    errors = lambda o, t: 0.5 * (o - t.T)**2\n",
      "    errors_prime = lambda o, t: o - t.T\n",
      "    \n",
      "    # define initial gradient step sizes and deltas to be used in delta_bar_delta\n",
      "    gamma_1 = np.random.random(n_features + 1).reshape(n_features + 1, 1) # (N + 1) x 1\n",
      "    bar_delta_1 = np.zeros((n_features + 1, 1)) # (N + 1) x 1\n",
      "    gamma_2 = np.random.random(k_hidden + 1).reshape(k_hidden + 1, 1) # (K + 1) x 1\n",
      "    bar_delta_2 = np.zeros((k_hidden + 1, 1)) # (K + 1) x 1\n",
      "    \n",
      "    # define offset and create arrays to store sample error & sum of errors\n",
      "    offset = 0\n",
      "    sample_error = np.empty((x_samples,1))\n",
      "    sum_of_errors = np.empty((iterations,1))\n",
      "    \n",
      "    # while user chooses to continue\n",
      "    iterate = True\n",
      "    while iterate:\n",
      "        \n",
      "        # iterate for specified number of steps\n",
      "        for iteration in range(iterations):\n",
      "            \n",
      "            # loop through each sample\n",
      "            for sample in range(x_samples):\n",
      "\n",
      "                # --------------------------------------------------------------------------------\n",
      "                # step one: feed-forward computation\n",
      "                # --------------------------------------------------------------------------------\n",
      "\n",
      "                # get input vector which, for consistancy, we will call output_0 and add bias term\n",
      "                output_0 = data[sample,:][np.newaxis,:] # 1 x N\n",
      "                output_0_hat = np.hstack((output_0, np.ones((1, 1)))) # 1 x (N + 1)\n",
      "                \n",
      "                \n",
      "                # caculate output_1 and add bias term\n",
      "                output_1 = sigmoid(output_0_hat, W_1_bar) # 1 x K\n",
      "                output_1_hat = np.hstack((output_1, np.ones((1, 1)))) # 1 x (K + 1)\n",
      "                \n",
      "                # caculate output_2 \n",
      "                output_2 = sigmoid(output_1_hat, W_2_bar) # 1 x M\n",
      "\n",
      "                # caculate derivatives of output_1 and output_2 and store in diagonal matrices\n",
      "                D_1 = np.diagflat(sigmoid_prime(output_1)) # K x K\n",
      "                D_2 = np.diagflat(sigmoid_prime(output_2)) # M x M\n",
      "                \n",
      "                # convert output_2 to vectorized label\n",
      "                output_2 = np.where(output_2 == output_2.max(), 1, 0)\n",
      "                \n",
      "                # caculate derivatives of each error and store\n",
      "                e = errors_prime(output_2, labels[sample,:]) # 1 x M\n",
      "\n",
      "                # caculate sum of the sample's error\n",
      "                sample_error[sample] = np.sum(errors(output_2, labels[sample,:]))\n",
      "                \n",
      "                # --------------------------------------------------------------------------------\n",
      "                # step two: backpropagation to the output layer\n",
      "                # --------------------------------------------------------------------------------\n",
      "\n",
      "                # define backpropagated error of output layer\n",
      "                delta_2 = np.dot(D_2, e.T) # M x 1\n",
      "\n",
      "                # define error gradient of output layer\n",
      "                nabla_E_2 = np.dot(delta_2, output_1_hat).T # (K + 1) x M\n",
      "\n",
      "                # --------------------------------------------------------------------------------\n",
      "                # step three: backpropagation to the hidden layer\n",
      "                # --------------------------------------------------------------------------------\n",
      "\n",
      "                # define backpropagated error of hidden layer\n",
      "                delta_1 = np.dot(D_1, np.dot(W_2_bar[:-1,:], delta_2)) # K x 1\n",
      "\n",
      "                # define error gradient of hidden layer\n",
      "                nabla_E_1 = np.dot(delta_1, output_0_hat).T # (N + 1) x K\n",
      "\n",
      "                # --------------------------------------------------------------------------------\n",
      "                # step four: weight updates\n",
      "                # --------------------------------------------------------------------------------\n",
      "\n",
      "                # get gamma_1 and gamma_2 using delta-bar-delta\n",
      "                gamma_1, delta_1 = delta_bar_delta(gamma_1, nabla_E_1, bar_delta_1, phi=random.random())\n",
      "                gamma_2, delta_2 = delta_bar_delta(gamma_2, nabla_E_2, bar_delta_2, phi=random.random())\n",
      "\n",
      "                # update weights\n",
      "                W_1_bar = -gamma_1 * nabla_E_1 # (N + 1) x K\n",
      "                W_2_bar = -gamma_2 * nabla_E_2 # (K + 1) x M\n",
      "                \n",
      "            # add all sample errors to sum of errors\n",
      "            sum_of_errors[iteration + offset] = np.sum(sample_error)\n",
      "\n",
      "        # output graph of error vs. steps\n",
      "        plot(sum_of_errors)\n",
      "        xlabel('Number of Iterations')\n",
      "        ylabel('Error')\n",
      "        title('Sum of Errors vs. Iterations')\n",
      "        display.clear_output(wait=True)\n",
      "        display.display(show())\n",
      "        \n",
      "        # output prompt to continue or exit\n",
      "        response = raw_input('Continue Training Neural Network, [y]/n: ')\n",
      "        if response.lower() != 'n': \n",
      "            offset += iterations\n",
      "            sum_of_errors = np.vstack((sum_of_errors, np.empty((iterations,1))))\n",
      "        else: iterate = False\n",
      "        \n",
      "    # return tuple of weights W_1 and W_2\n",
      "    return W_1, W_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the training data from file\n",
      "with gzip.open('../Data/zip.train.gz') as f:\n",
      "    train = np.loadtxt(f)\n",
      "    \n",
      "# separate the observations from the labels and vectorize the labels\n",
      "X_train = train[:,1:]\n",
      "y_train, num_labels = vectorize_digit_labels(train[:,0][:,np.newaxis])\n",
      "\n",
      "# reduce the dimensions of the data to 20\n",
      "X_train = pca(X_train, 20)\n",
      "\n",
      "# train weights using single hidden layer neural network\n",
      "W_1, W_2 = single_hidden_train(X_train, y_train, num_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEZCAYAAAB8culNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5FIOUQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10c5e1590>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "None"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Continue Training Neural Network, [y]/n: n\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}