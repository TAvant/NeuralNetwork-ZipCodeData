{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_digit_labels(labels):\n",
    "    '''name:         vectorize_digit_labels\n",
    "       description:  function vectorizes the labels of digits (0-9), for example: \n",
    "                         0 == [1,0,0,0,0,0,0,0,0,0]\n",
    "                         .\n",
    "                         .\n",
    "                         .\n",
    "                         5 == [0,0,0,0,0,1,0,0,0,0]\n",
    "                         .\n",
    "                         .\n",
    "                         .\n",
    "                         9 == [0,0,0,0,0,0,0,0,0,1]\n",
    "       dependencies: none\n",
    "       inputs:       labels - N x 1 vector of digit (0-9) labels\n",
    "       outputs:      labels_vectorized - N x 10 matrix of vectorized digits\n",
    "                     num_unique - number of unique labels\n",
    "    '''\n",
    "    \n",
    "    # get number of samples and number of unique labels\n",
    "    samples = labels.shape[0]\n",
    "    num_unique = unique(labels).shape[0]\n",
    "    \n",
    "    # create a matrix of unique vectos corresponding to unique labels\n",
    "    unique_labels_vectorized = eye(num_unique)\n",
    "    \n",
    "    # create an empty matrix to store vectorized labels\n",
    "    labels_vectorized = empty((samples, num_unique))\n",
    "    \n",
    "    # get vectorized version of labe for each sample\n",
    "    for sample_idx in range(samples):\n",
    "        \n",
    "        # using label as index, because labels are digits (0-9)\n",
    "        label_idx = labels[sample_idx].astype(int)\n",
    "        \n",
    "        # adding vectorized version to matrix of labels\n",
    "        labels_vectorized[sample_idx] = unique_labels_vectorized[label_idx,:]\n",
    "        \n",
    "    # return vectorized labels\n",
    "    return labels_vectorized, num_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pca(data, k_features):\n",
    "    '''name:         pca\n",
    "       description:  function takes an original data set an makes the following transformations: \n",
    "                     the data is centered about the origin; the covariance is then calculated; \n",
    "                     the eigenvalues and eigenvectors of the covariance are found; \n",
    "                     the original data is the projected onto the k eigenvectors in descending order \n",
    "                     of their eigenvalues, creating a new N x K matrix of k principal components\n",
    "       dependencies: none\n",
    "       inputs:       data - is an N x K matrix with the rows representing observations and columns representing features\n",
    "                     k_features - is an integer representing the number of principal components or features to keep\n",
    "       outputs:      reduced_data - an N x k_features matrix \n",
    "    '''\n",
    "    \n",
    "    # check 0 < k_features <= number of features\n",
    "    if k_features > 0 and k_features <= data.shape[1]:\n",
    "\n",
    "        # center the data and calculate the covariance matrix (sigma)\n",
    "        sigma = corrcoef(data.T)\n",
    "\n",
    "        # get the eigenvectors of sigma\n",
    "        eigen_vecs, _, _ = linalg.svd(sigma)\n",
    "\n",
    "        # create an empty matrix to hold dimensionally reduced data\n",
    "        reduced_data = empty((data.shape[0], k_features))\n",
    "\n",
    "        # for each observation x, project x onto eigenvectors\n",
    "        for observation_idx in range(data.shape[0]):\n",
    "            reduced_data[observation_idx] = dot(eigen_vecs[:,:k_features].T, data[observation_idx,:][:,newaxis])[:,newaxis].T\n",
    "\n",
    "        # return dimensionally reduced data\n",
    "        return reduced_data\n",
    "    \n",
    "    # print error message\n",
    "    print ('ERROR: 0 < k_features < %i') % data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delta_bar_delta(gamma, nabla_E, delta, up=0.1, down=0.1, phi=0.01):\n",
    "    '''name:         delta_bar_delta\n",
    "       description:  step size (gamma) is increased whenever the algorithm proceeds down the error function\n",
    "                     step size (gamma) is decreased when the algorithm jumps over a valley of the error function\n",
    "       dependencies: none\n",
    "       inputs:       gamma - vector of step sizes\n",
    "                     nabla_E - matrix of gradient errors\n",
    "                     delta - vector of exponentially averaged partial derivative in the direction of weight\n",
    "       outputs:      gamma - vector of step sizes\n",
    "                     delta_new - updated delta\n",
    "    '''\n",
    "    \n",
    "    # caculate sum of gradient error\n",
    "    nabla_E_sum = sum(nabla_E, axis=1)[:,newaxis]\n",
    "    \n",
    "    # caculate new delta\n",
    "    delta_new = (1 - phi) * nabla_E_sum + phi * delta\n",
    "    \n",
    "    # update each gamma\n",
    "    for idx in range(gamma.shape[0]):\n",
    "        \n",
    "        # if gradient is on same side, from previous, increase the step size\n",
    "        if nabla_E_sum[idx] * delta[idx] > 0: gamma[idx] + up\n",
    "        \n",
    "        # if gradient is on opposite side, form previous, increase the step size\n",
    "        elif nabla_E_sum[idx] * delta[idx] < 0: gamma[idx] * down\n",
    "            \n",
    "        # otherwise do nothing\n",
    "        else: pass\n",
    "            \n",
    "    # return modified gamma and new delta\n",
    "    return gamma, delta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def single_hidden_train(data, labels, m_output, k_hidden=20, iterations=10):\n",
    "    '''name:         single_hidden_train\n",
    "       description:  function learns the weights of a single hidden layer neural network \n",
    "                     using a back propagation algorithm defined as:\n",
    "                         1. Feed-forward computation\n",
    "                         2. Backpropagation to the output layer\n",
    "                         3. Backpropagation to the hidden layer\n",
    "                         4. Weight updates using gradient descent\n",
    "                     in addition, the function dispalys a plot of the error verses the number of\n",
    "                     iterations; at this point the user may decide to continue the training or\n",
    "                     exit the function\n",
    "       dependencies: delta_bar_delta\n",
    "       inputs:       data - an N x K matrix with the rows representing observations and columns representing features\n",
    "                     labels - matrix of vectorized labels\n",
    "                     k_units - number of hidden units\n",
    "                     m_output - number of output labels\n",
    "                     iterations - number of times the weights are updated \n",
    "       outputs:      W_1 - trained weights for input layer\n",
    "                     W_2 - trained weights for output of hidden layer\n",
    "    '''\n",
    "    \n",
    "    # get number of samples and features\n",
    "    x_samples = data.shape[0]\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    # intialize initial vectors of weights, with bias term added to last row\n",
    "    W_1_bar = random.uniform(0.01, 0.1, (n_features + 1, k_hidden)) # (N + 1) x K\n",
    "    W_2_bar = random.uniform(0.01, 0.1, (k_hidden + 1, m_output)) # (K + 1) x M\n",
    "    \n",
    "    # define node functions sigmoid and sigmoid_prime\n",
    "    sigmoid = lambda vec_x: 1 / (1 + exp(-vec_x))\n",
    "    sigmoid_prime = lambda vec_x: sigmoid(vec_x) * (1 - sigmoid(vec_x))\n",
    "    \n",
    "    # define error function\n",
    "    error_func = lambda vec_e: 0.5 * vec_e**2\n",
    "    \n",
    "    # define initial gradient step sizes and deltas to be used in delta_bar_delta\n",
    "    gamma_1 = random.uniform(0.01, 0.1, (k_hidden, 1)) # K x 1\n",
    "    bar_delta_1 = zeros((k_hidden, 1)) # K x 1\n",
    "    gamma_2 = random.uniform(0.01, 0.1, (m_output, 1)) # M x 1\n",
    "    bar_delta_2 = zeros((m_output, 1)) # M x 1\n",
    "    \n",
    "    # define offset and create arrays to store sample error & sum of errors\n",
    "    offset = 0\n",
    "    sample_error = zeros((x_samples,1))\n",
    "    sum_of_errors = zeros((iterations,1))\n",
    "    \n",
    "    # while user chooses to continue\n",
    "    iterate = True\n",
    "    while iterate:\n",
    "        \n",
    "        # iterate for specified number of steps\n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            # loop through each sample\n",
    "            for sample_idx in range(x_samples):\n",
    "\n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step one: feed-forward computation\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # get input vector which, for consistancy, we will call output_0 and add bias term\n",
    "                output_0 = data[sample_idx,:][newaxis,:] # 1 x N\n",
    "                output_0_hat = hstack((output_0, ones((1, 1)))) # 1 x (N + 1)\n",
    "                \n",
    "                # caculate output_1 and add bias term\n",
    "                output_1 = sigmoid(dot(output_0_hat, W_1_bar)) # 1 x K\n",
    "                output_1_hat = hstack((output_1, ones((1, 1)))) # 1 x (K + 1)\n",
    "                \n",
    "                # caculate output_2 and convert to vectorized label\n",
    "                output_2 = sigmoid(dot(output_1_hat, W_2_bar)) # 1 x M\n",
    "                new_output_2 = zeros(output_2.shape)\n",
    "                new_output_2[:,argmax(output_2)] = 1\n",
    "                \n",
    "                # caculate derivatives of output_1 and output_2 and store in diagonal matrices\n",
    "                D_1 = diagflat(sigmoid_prime(dot(output_0_hat, W_1_bar))) # K x K\n",
    "                D_2 = diagflat(sigmoid_prime(dot(output_1_hat, W_2_bar))) # M x M\n",
    "                \n",
    "                # caculate difference in error\n",
    "                error_diff = new_output_2 - labels[sample_idx,:][newaxis,:] # 1 x M\n",
    "                \n",
    "                # caculate sum of the sample's error\n",
    "                sample_error[sample_idx] = sum(error_func(error_diff))\n",
    "                \n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step two: backpropagation to the output layer\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # define backpropagated error of output layer\n",
    "                delta_2 = dot(D_2, error_diff.T) # M x 1\n",
    "                \n",
    "                # define error gradient of output layer\n",
    "                nabla_E_2 = dot(delta_2, output_1_hat) # M x (K + 1)\n",
    "                \n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step three: backpropagation to the hidden layer\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # define backpropagated error of hidden layer\n",
    "                delta_1 = dot(D_1, dot(W_2_bar[:-1,:], delta_2)) # K x 1\n",
    "                \n",
    "                # define error gradient of hidden layer\n",
    "                nabla_E_1 = dot(delta_1, output_0_hat) # K x (N + 1)\n",
    "                \n",
    "                # --------------------------------------------------------------------------------\n",
    "                # step four: weight updates\n",
    "                # --------------------------------------------------------------------------------\n",
    "\n",
    "                # get gamma_1 and gamma_2 using delta-bar-delta\n",
    "                gamma_1, bar_delta_1 = delta_bar_delta(gamma_1, nabla_E_1, bar_delta_1)\n",
    "                gamma_2, bar_delta_2 = delta_bar_delta(gamma_2, nabla_E_2, bar_delta_2)\n",
    "                \n",
    "                # update weights\n",
    "                W_1_bar += (-gamma_1 * nabla_E_1).T # (N + 1) x K\n",
    "                W_2_bar += (-gamma_2 * nabla_E_2).T # (K + 1) x M\n",
    "                \n",
    "            # add all sample errors to sum of errors\n",
    "            sum_of_errors[iteration + offset] = sum(sample_error) / x_samples\n",
    "            \n",
    "        # output graph of error vs. steps\n",
    "        plot(sum_of_errors)\n",
    "        xlim([0,sum_of_errors.shape[0]-1])\n",
    "        ylim([0,sum_of_errors[argmax(sum_of_errors)]])\n",
    "        xlabel('Number of Iterations')\n",
    "        ylabel('Error Percentage')\n",
    "        title('Sum of Errors vs. Iterations')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(show())\n",
    "        \n",
    "        # output prompt to continue or exit\n",
    "        response = raw_input('Continue training neural network ([y]/n)? ')\n",
    "        if response.lower() != 'n': \n",
    "            offset += iterations\n",
    "            sum_of_errors = vstack((sum_of_errors, zeros((iterations,1))))\n",
    "        else: iterate = False\n",
    "        \n",
    "    # return weights with bias row removed\n",
    "    return W_1_bar[:-1,:], W_2_bar[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEZCAYAAAB8culNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYXVWd7vHvmwRkHgQUgTCDCAhEAVEcgtIaoIFW2xbE\n",
       "GW26FbW7HdB7VWi1G73q1au0tgPIYBC7ERVtlUmiKAiEeUiYJAIBARmDCATy3j/2KrJT51TVqdQ5\n",
       "tU9VvZ/n2U+dPf9OVXJ+Z62111qyTURERN20pgOIiIj+k+QQEREtkhwiIqJFkkNERLRIcoiIiBZJ\n",
       "DhER0SLJISYUSc+VdKWkhyUd2XQ8AZJeJmlh03FEdyU5RFuSXirpQkkPSrpP0m8k7d50XMBHgPNs\n",
       "r2P7uME7Jc2T9BdJS2rLjxuIc1xI2lLSMknTyvqJkj7d43suk7T1wLrtC2zv0Mt7xvib0XQA0X8k\n",
       "rQP8FDgC+C/gGcDLgMebjKvYArhwmP0G3mv7hJEuJGmG7ScHbZtu+6lOgxnt8f1uFO9HPQ8mGpWS\n",
       "Q7SzPWDb33flMdvn2L4GQNIxkk4ZOLjNt9d5kj4t6bflm/uZkjaUNFfSQ5IukbTFUDeXdJCk6yQ9\n",
       "IOl8STuU7b8EZgPHlWqlbUfzpiTNlnSHpI9Iugs4QdLRkk6XdIqkh4C3SdqkxHyfpJskvat2jWPa\n",
       "HL+npPnlvf1R0heHuP8CSQfU1mdIulfSbpJWk/RdSX8q7/sSSc8a5fv7e+BNwEfqJabyfn4g6R5J\n",
       "v5f0vhHezx6SLipx3Cnpq5JWKcf/upx6VbnHG8rv9fbaNZ9X/g08IOlaSQfW9p0o6T8k/bT8DX9X\n",
       "L4VI+pKku8vv8mpJO43mdxBdZDtLlhUWYG3gT8CJwBxg/UH7jwZOqa1vCSwDppX1ecCNwFbAOsB1\n",
       "wE3AK4HpwEnACUPce3vgEeBV5dgPl3NnlP3nA+8cJvbzgcOH2DcbWAocC6wCrAYcAzwBHFSOWQ34\n",
       "NXAcsCqwK3APsE/Z3+74i4DDyvoawIuGuP8ngO/W1g8AriuvjwDOLNcTMAtYu4O/1eDf/XeAT9X2\n",
       "TwMuAz5OVVOwFXAL8Oph3s8LgD3LuVsA1wMfqF1zGbD1oN/r7eX1KsDNwEfL/fYBHga2L/tPpPq3\n",
       "tXv5+34X+F7Z9xpgPrBOWX8usHHT/x+m6pKSQ7SwvQR4KVUVzbeAeyT9uPZNdqQqBQPfsX2r7YeB\n",
       "nwM32v6lqyqL/6b68GvnjcBPbZ9Xjv0CsDrwktoxw91fwFfKt9aB5V9r+5cBR9teavuxsu1C22eW\n",
       "1xuVex1l+wnbVwHfBt5au8bTx5drPAFsJ2lD24/avniI2E4FDpK0Wll/E/C98voJYANgO1euKH+H\n",
       "lVH//ewBbGj7M7aftH1reT+HDPV+bF9u+xLby2z/Afgm8IoO770XsKbtz5b7nU9VRXlo7ZgzbM8v\n",
       "f9+5wG5l+1KqLybPkzTN9g22/zjK9x5dkuQQbdleaPsdtmcCOwObAF8exSXurr1+jOrbd319rSHO\n",
       "ew5wWy0OA7cDm9bDGy504H22168tR9f232v7iUHn3FF7vQlwv+0/17bdNuj+9eMBDqcq8Swo1UEH\n",
       "0IbtW4AFVAliDeBAqoQBcApwFnCapMWSPiepG22CWwCb1JMl8DGgXmW1wvuRtH2p9rmrVDX9G1Xi\n",
       "6sQmVH+vuj+U7VD9fer/Nv5C+bdg+5dUJbb/AO6W9A1Ja3d43+iyJIcYke0bqKqCdi6b/kxVfTJg\n",
       "45EuMYrb3Un1gQaAJAEzgcWjuMZoYvGgbXcCz5RUT16bs+IH6ArXsH2z7TfZ3gj4HHC6pNWHuP/3\n",
       "qL5FH0xVpfT7co0nbX/K9k5UJZe/ZsXSSqcGv7/bgFsHJct1bP917fjB53ydqippW9vrAv+bzj8r\n",
       "7gRmlr/bgC3o8O9n+6u2dwd2pEq4H+7wvtFlSQ7RQlVfgn+RtGlZn0n1gXZROeRK4OWSZkpal+qb\n",
       "aMtlhng9kv8CDpD0ytII+kGqkkb9CaWRrjea+61wrO3by72OlfQMSbsA76SqG29/AenNkjYqqw9R\n",
       "fdguG+Lw06jq1v+B5aWGgcby50uaDiyhqmJZmaeg7ga2rq1fAiwpjfCrS5ouaWctfyy53e9qrRLD\n",
       "o+VhgH9sc49thrj/xcCjVI3iq0iaTZXoThvmftUOaXdJLyp/90ep/u6T5kmwiSbJIdpZArwIuFjS\n",
       "I1RJ4WqqD2psnwN8v2y7FPgJ7b+R118Pt3/5RvtG4M3AV4F7qRptD/SKj5yOVBI5Tiv2c7h0mHPb\n",
       "xXYoVUPvncAZwCdLlcdQx78GuFbSEuBLwCG22z72W+rQLwReTPU7HLAxVVvMQ1Tf2udRVTUh6euS\n",
       "vj7M+63HczywY6lCOsP2MqoP592A31P9Tr9J9aDAUO/nQ1TtIQ+XY08bdMwxwEnlHn9bv0apsjsQ\n",
       "2K/c6zjgLeXvOtT9BtbXKfe7H1hE1XD9+WHed/SQqirdiIiI5VJyiIiIFkkOERHRIskhIiJaJDlE\n",
       "RESLCT3wnqS0pkdErATbwz/y3fT4HWNZAIP/uek4BsV0TNMxTJS4ElNimgpx9WlMHumYyVCt9Lam\n",
       "A4iImGwmQ3JYX3p64K6IiOiCyZAcTqa/Sg/zmg5gCPOaDqCNeU0H0Ma8pgNoY17TAbQxr+kAhjCv\n",
       "6QDamNd0ACtjQveQrhqkvT3wG2Azm6VNxxQR0e8k2SM0SE/4koPNTVSTi8xpOpaIiMliwieH4iT6\n",
       "q2opImJCm/DVSrYlsR7VKI7b2NzXcFgREX1tSlQrAdg8SDUV5SEjHRsRESObFMmhSNVSRESXTKbk\n",
       "cA6wqcSOTQcSETHRTZrkYPMU1VSOKT1ERIzRpGiQXr7OjlQliM1LsoiIiEGmTIP0AJvrqeb93bfp\n",
       "WCIiJrJJlRyKE4G3NxxDRMSENqmqlaptbADcAmxh81AzkUVE9K8pV60EUDrBnQf8XdOxRERMVJMu\n",
       "ORQnkaqliIiVNlmTw8+BbSW2azqQiIiJaFImhzJ096nAW5uOJSJiIpp0DdLL97Eb8GNgK5tl4xtZ\n",
       "RET/mpIN0gNsrgQeAGY3HEpExIQzaZNDkcH4IiJWwqStVqr282zgBqopRB8Zv8giIvpX49VKkuZI\n",
       "WijpJklHtdl/mKSrJF0t6beSdqntW1S2XyHpkpW5v83dwAXA61f+XURETD09KzlImk71rX1fYDFw\n",
       "KXCo7QW1Y14MXG/7IUlzgGNs71X23Qq80Pb9w9xjxOwn8XrgSJt9xvymIiImgaZLDnsCN9teZHsp\n",
       "cBpwcP0A2xfZHhji4mJgs0HXGDb4Dv0UeL7Ell24VkTElNDL5LApcHtt/Y6ybSiHAz+rrRs4V9J8\n",
       "Se9e2SBsHge+D7xlZa8RETHVzOjhtTuur5K0D/BOYO/a5r1t3yVpI+AcSQttX7CSsZwEnCrxGbvz\n",
       "uCIipqpeJofFwMza+kyq0sMKSiP0t4A5th8Y2G77rvLzXkk/pKqmakkOko6prc6zPa9NLJcCT1Al\n",
       "n9+M9o1ERExkkmYzyj5fvWyQnkHVIP0qqgl4LqG1QXpz4JfAm23/rrZ9DWC67SWS1gTOBv7V9tmD\n",
       "7jFio8ryYzkK2NZmpauoIiImg44e5ullPwdJ+wFfBqYDx9s+VtIRALa/IenbwGuB28opS23vKWlr\n",
       "4IyybQYw1/axba4/muSwKXANVZ+HR8f0xiIiJrDGk0OvjSY5VMfzC+Bkm1N7GFZERF9r+lHWfpTh\n",
       "NCIiOjDVSg6rUzWUP99mce8ii4joXyk5DGLzF+AHpM9DRMSwplRyKE4E3iZ1pfd1RMSkNBWTw4VU\n",
       "T0Dt0XQgERH9asolh9JD+mTSMB0RMaQp1SC9/Dy2BC4DNiljL0VETBlpkB6CzSLgKuDAhkOJiOhL\n",
       "UzI5FOnzEBExhClZrVSdy1pUAwE+t8wYFxExJaRaaRhlTukfAYc1HUtERL+ZssmhSNVSREQbUz05\n",
       "/ApYV2K3pgOJiOgnUzo52CwDTgHe3nAoERF9Zco2SC+/BtsCv6Wa52FpdyKLiOhfaZDugM3NwE3A\n",
       "fk3HEhHRL6Z8cijSMB0RUTPlq5Wq67Au1VSlW9vcN/bIIiL6V6qVOmTzEPA/wKFNxxIR0Q+SHJZL\n",
       "1VJERJHksNy5wCYSOzUdSERE05IcCpunqPo8pPQQEVNeGqRXuB7PA84DNrd5slvXjYjoJ2mQHiWb\n",
       "BVQjte7bdCwREU1Kcmh1EhlOIyKmuFQrtVyTDYDfA1vYPNjNa0dE9INUK62E0gnuHODvmo4lIqIp\n",
       "SQ7tpWopIqa0JIf2fgFsI7F904FERDQhyaGNMnT3XOCtTccSEdGEniYHSXMkLZR0k6Sj2uw/TNJV\n",
       "kq6W9FtJu3R67jg4CXirlAQaEVNPzz74JE0HjgPmADsCh0p63qDDfg+83PYuwKeBb47i3J6yuQq4\n",
       "H9hnPO8bEdEPevmteE/gZtuLbC8FTgMOrh9g+yLbD5XVi4HNOj13nJxIhtOIiCmol8lhU+D22vod\n",
       "ZdtQDgd+tpLn9sqpwEESazdw74iIxszo4bU77l0naR/gncDeK3HuMbXVebbndXruSGzukfg18Hqq\n",
       "UkRExIQjaTYwezTn9DI5LAZm1tZnUpUAVlAaob8FzLH9wGjOBbB9TDeCHcZJwPtIcoiICap8aZ43\n",
       "sC7p6JHO6WW10nxgO0lbSloVeCNwZv0ASZsDZwBvtn3zaM4dRz8FdpbYqqH7R0SMuxGTg6TnSjpP\n",
       "0nVlfRdJHx/pPNtPAkcCZwHXA9+3vUDSEZKOKId9Elgf+LqkKyRdMty5K/H+xszmcaoG8bc0cf+I\n",
       "iCaMOPCepF8DHwb+0/YsSQKutd34jGm9GHiv/X3YgypBbGt33h4SEdGPujXw3hq2Lx5YcZVNlo41\n",
       "uAlmPvA48NKmA4mIGA+dJId7JW07sCLpb4G7ehdS/ymlhRNJn4eImCI6qVbahqrn8kuAB4BbgcNs\n",
       "L+p5dCMYr2ql6l5sAlwHbGrz6HjcMyKiFzr57Ox4sh9JawLTbC/pRnDdMJ7JobofvwBOsZk7XveM\n",
       "iOi2riQHSR+ktVPaQ8Bltq8cW4hj00ByOAR4p82rx+ueERHd1q3kcCqwO/ATQMABwDXAFsDptj/X\n",
       "nXBHr4HksDpVB71d7Pad8iIi+l23ksMFwH62Hynra1GNgTSHqvQwrqOlDoptXJNDdU++Cdxqc+x4\n",
       "3jciolu69SjrRsATtfWlwLNtPwo8Nob4JqqTgLdJjGtSiogYT52MrTQXuFjSj6iqlQ4ETi0N1Nf3\n",
       "Mrg+dSEwnWpY8YtHODYiYkLq6GklSXtQjZhq4Le25/c6sE40Ua1U3ZePA5vYvGe87x0RMVbdfpT1\n",
       "2cBqlCeXbN825gjHqMHksAVwOVWfh6lYtRYRE1hX2hwkHSTpJqopPecBi4CfdyPAicrmD8BVVFVs\n",
       "ERGTTicN0p8BXgzcaHsr4FWkrh0ynEZETGKdJIeltv8ETJM03fb5VP0eprozgL0lNm46kIiIbusk\n",
       "OTwgaW3gAmCupK8Aj/Q2rP5n8wjwY+CwpmOJiOi2TjrBrUnVn2Ea1QfhOsBc2/f1PrzhNdUgvfz+\n",
       "zAa+AuyaeR4iYqLoVie4T9p+yvZS2yfa/grwke6EOOH9mipZ7tZ0IBER3dRJcmg3yNz+3Q5kIrJZ\n",
       "BpwMvL3hUCIiumrIaiVJ/wi8B9gGuKW2a22qjnCN17U3Xa1UxcC2VL2mN7NXGGYkIqIvjakTnKR1\n",
       "gfWBzwJHwdNjCS3ph/YG6I/kUMXBBcAXbH7cdCwRESPpWg9pSdOBZ1Mbi2kq95BujYN3AfvbvK7p\n",
       "WCIiRtKtIbvfBxwN3AM8NbDd9vO7EeRY9FFyWBe4DdjG5k9NxxMRMZxuJYdbgD37pSqprl+SA4DE\n",
       "qcBFNl9tOpaIiOF061HW24CHuxPSpHYiGU4jIiaJTuZzuBU4X9L/sHzSH9v+v70La0I6D9hYYmeb\n",
       "a5sOJiJiLDotOZwLrAqsVZa1exnURGTzFPBdUnqIiElgNPM5rGn7zz2OZ1T6qc0BQGIH4HfAWVTD\n",
       "mp9lc1ezUUVErKhb8zm8RNL1wMKyvqukr3UpxknFZiGwE1Vy2B+4TuJKic9KzJZYtdkIIyI608nT\n",
       "SpcAfwv82Passu062zuNQ3zD6reSw2ASM6jmmp4D7AdsD5zP8lLFouaii4ipqltPK7Xr8PbkSkc1\n",
       "hdg8aXOhzSdt9gC2A/4beClwicQCiS9JvEZi9WajjYhYrqMGaUl7A0haVdKHgAWdXFzSHEkLJd0k\n",
       "6ag2+3eQdJGkxyR9cNC+RZKulnRFKb1MeDb32My1eQuwMfBm4E/AJ4B7JH4u8QGJ50r0bYkoIia/\n",
       "TqqVNgL+H7Av1fhKZwPvH6lTXBly44Zy3mLgUuBQ2wtqx2wEbAH8DfCA7S/W9t0KvND2/cPco6+r\n",
       "lUZDYn2qKVjnlOUJ4BdlOd9mSYPhRcQk0sln54j9HGzfC7xpJe6/J3Cz7UUlmNOAg6mVOsq175V0\n",
       "wBDXmBQf/J2weQA4HTi9lBp2okoS7wfmSlzK8mRxTSYXiohe6uRppZMlrVdbX1/SCR1ce1Pg9tr6\n",
       "HWVbpwycK2m+pHeP4rwJz8Y219p8wWZf4DnAl6hKWT8C7pA4XuINpcQREdFVnfSQ3sX2gwMrth+Q\n",
       "9IIOzhvrN9u9bd9Vqp7OkbTQ9gWDD5J0TG11nu15Y7xv3ynzVf8E+EkpVWxLVap4O3C8xDUsL1Vc\n",
       "ViYhiogAQNJsYPZozukkOUjSMwfq/iU9E5jewXmLgZm19ZlUpYeO2L6r/LxX0g+pqqlakoPtYzq9\n",
       "5mRQqpNuKstXy1NOL6NKFicBG0qcTZUozra5p7FgI6IvlC/N8wbWJR090jmdPK30ReAiSZ+W9Bng\n",
       "IuDzHZw3H9hO0paSVgXeCJw5xLErtC1IWkPS2uX1mlRTlV7TwT2nHJu/2Jxt8y82OwJ7UM1t/Trg\n",
       "Ron5Ev8mcZDEZnkKKiI6MezTSpKmAS8GHgReSVVV9Evb13d0cWk/4MtUJY3jbR8r6QgA29+QtDHV\n",
       "U0zrAMuAJcCOwLOAM8plZgBzbR/b5vqT5mmlXig9sl8M/BWwO/ACqkR8OXBF7ectqYqKmDq6NZ/D\n",
       "lbZ362pkXZLkMDql1LAJVZKYVfu5HnAlKyaMBXY6O0ZMRt1KDl+gGkzuB+50lL5xkuTQHRIbArtR\n",
       "JYuBhLEZcB0rljKusXmsqTgjoju6lRweAdagmiJ04IPBttfpSpRjkOTQOxJrA7uyYiljO+BmVkwY\n",
       "V9mZDCpiIulKcuhnSQ7jS2I1YGdWTBg7Uz2ZNpAsLgeuyFzaEf2rWyWHacBhwFa2PyVpc2Bj242P\n",
       "d5Tk0Lwy8uwOrNiGMQt4iBXbMC4HFqdnd0TzupUc/pPqSaJX2t6h9HM42/bu3Qt15SQ59CeJacBW\n",
       "rFjCGPyk1BXAvcBSqnGklg6z1Pc/lQQTMTbdSg5X2J418LNsu8r2rl2MdaUkOUwcbZ6UmgWsD6wy\n",
       "wrLqoPVpjJxAOkkyQy33AjdSdTK8xebxnvxCIhrUlYH3gCfKCKsDF90I8kx8jE75tr+4LD9Z2euU\n",
       "UslICaSTJDPUMc8BXkE1MdMWEoupksXg5fYyb3jEpNRJcvgq8EPgWZL+nWpWuI/3NKqIIZTOeo+X\n",
       "packVqGqHtu+LDsBry2vN5S4heXJ4qba63tS9RUTXUdPK0nagWpeBoDz6nMyNCnVStEUiTWpBkDc\n",
       "vrZsV36uQvvSxk157Df6wZjaHCTtBXyD6j/A1cDhnQ6bMV6SHKIfSWzA8kQxOHksoX3iSPtGjJux\n",
       "JofLgI9SjYR6IPAu26/pepRjkOQQE0mtUX5w0tieaq6OOxlU0mB5+8akHsqk/G7WoBpnbe0Ol8HH\n",
       "rg78gapn/9NLSmutxpocnn46qd16P0hyiMmitG9sSfsSx6ZUg14+xvL2lsdHWB/NsaM990kbl4Ed\n",
       "V/aDfPCyVrn2ktry8KD1wcvg/Y8BW1O1DQ0szwPuZ1DCAK6fylPvjjU5/B74EMuH0/58bd22z2h7\n",
       "4jhKcoiponQ2XA14Rlk6ed3pcaM9B6rhdKYz+g/wofY90ovSUXm6bUtWTBg7UXXcvJf2SePP3Y6j\n",
       "34w1OZzIirO5qb5u+x1diHFMkhwixl9JVKsAj03Up7IkplM9iTY4aWwP3E1r0lhg82gz0XZfxlaK\n",
       "iBiFkvgGV03tRFXddyetSWOhzV+aiXblJTlERHRBSRrb0po0tqGa/nhw0rihn4e3T3KIiOih8iBB\n",
       "u6SxNXAbVaK4EbilttzRdO/6MSeHMiLrXrYv7HZw3ZDkEBH9qDzJtR3Lq6S2qS0bUj1ye0ub5dbx\n",
       "qKbKNKEREX1GYnWqxvBt2ixbAH+ifeK4xeb+7sSQaUIjIiaM8hTVZrRPHNtQDXraNnFQzZfS0aCo\n",
       "mSY0ImKSKL3IN2DoxLE+sIihq6seX36tNEhHREwJEmtQNYS3SxwzgXt4Olno8K4kB0kHAy+n6gT3\n",
       "K9srPR5/NyU5RESMrDyKO5Onk4X+sxvVSp8F9gDmUvWSPgSYb/tjXYl6DJIcIiJGr1ttDtcAu9l+\n",
       "qqxPB660/fyuRbqSkhwiIkavk8/OaR1cx8B6tfX1YGKOpxIREZ3pZJrQY4HLJZ1PVa30Cqp5HiIi\n",
       "YpIaNjmUHtLLgBdTtTsY+Kjtu8YhtoiIaEgnbQ6X2X7hOMUzKmlziIgYvW61OZwj6UOSZkp65sDS\n",
       "YQBzJC2UdJOko9rs30HSRZIek/TB0ZwbERG900nJYRGtDdC2vfUI500HbgD2BRYDlwKH2l5QO2Yj\n",
       "qrFE/gZ4wPYXOz23HJeSQ0TEKI255FDaHI6yvdWgZdjEUOwJ3Gx7ke2lwGnAwfUDbN9rez6wdLTn\n",
       "RkRE7wybHGwvAz6yktfeFLi9tn5H2dbrcyMiYox62eYwlr4Q6UcREdGgTvo5HEL1Yf3eQdu3GuG8\n",
       "xVRjeQyYSVUC6ETH50o6prY6z/a8Du8RETElSJoNzB7VOb0alVXSDKpG5VdRTcx9CW0alcuxxwBL\n",
       "ag3SHZ2bBumIiNEbU4O0pI/UXr9h0L5/H+nmtp8EjgTOAq4Hvm97gaQjJB1RrrOxpNuBfwY+Luk2\n",
       "SWsNde5I94yIiO4YsuQg6Qrbswa/brfelJQcIiJGr1ud4CIiYopJcoiIiBbDVSs9BTxaVlcH/lLb\n",
       "vbrtTp506qlUK0VEjF4nn51DfsDbnt79kCIiYiJItVJERLRIcoiIiBZJDhER0SLJISIiWiQ5RERE\n",
       "iySHiIhokeQQEREtkhwiIqJFkkNERLRIcoiIiBZJDhER0SLJISIiWiQ5REREiySHiIhokeQQEREt\n",
       "khwiIqJFkkNERLRIcoiIiBZJDhER0SLJISIiWiQ5REREiySHiIhokeQQEREtkhwiIqJFkkNERLRI\n",
       "coiIiBY9TQ6S5khaKOkmSUcNccxXyv6rJM2qbV8k6WpJV0i6pJdxRkTEimb06sKSpgPHAfsCi4FL\n",
       "JZ1pe0HtmP2BbW1vJ+lFwNeBvcpuA7Nt39+rGCMior1elhz2BG62vcj2UuA04OBBxxwEnARg+2Jg\n",
       "PUnPru1XD+OLiIgh9DI5bArcXlu/o2zr9BgD50qaL+ndPYsyIiJa9KxaierDvRNDlQ5eavtOSRsB\n",
       "50haaPuClpOlY2qr82zPG12YERGTm6TZwOzRnNPL5LAYmFlbn0lVMhjumM3KNmzfWX7eK+mHVNVU\n",
       "LcnB9jHdCzkiYvIpX5rnDaxLOnqkc3pZrTQf2E7SlpJWBd4InDnomDOBtwJI2gt40PbdktaQtHbZ\n",
       "vibwauCaHsYaERE1PSs52H5S0pHAWcB04HjbCyQdUfZ/w/bPJO0v6Wbgz8A7yukbA2dIGohxru2z\n",
       "exVrRESsSHanTQP9R5Jt54mmiIhR6OSzMz2kIyKiRZJDRES0SHKIiIgWSQ4REdEiySEiIlokOURE\n",
       "RIskh4iIaJHkEBERLZIcIiKiRZJDRES0SHKIiIgWSQ4REdEiySEiIlokOURERIskh4iIaJHkEBER\n",
       "LZIcIiKiRZJDRES0SHKIiIgWSQ4REdEiySEiIlokOURERIskh4iIaJHkEBERLZIcIiKiRZJDRES0\n",
       "SHKIiIgWSQ4REdEiySEiIlr0NDlImiNpoaSbJB01xDFfKfuvkjRrNOdGRERv9Cw5SJoOHAfMAXYE\n",
       "DpX0vEHH7A9sa3s74O+Br3d6br+SNLvpGNrpx7gSU2cSU+f6Ma5+jKkTvSw57AncbHuR7aXAacDB\n",
       "g445CDgJwPbFwHqSNu7w3H41u+kAhjC76QDamN10AG3MbjqANmY3HUAbs5sOYAizmw6gjdlNB7Ay\n",
       "epkcNgVur63fUbZ1cswmHZwbERE90svk4A6PUw9jiIiIlTCjh9deDMysrc+kKgEMd8xm5ZhVOjgX\n",
       "AEmdJqFxI+nopmNopx/jSkydSUyd68e4+jGmkfQyOcwHtpO0JXAn8Ebg0EHHnAkcCZwmaS/gQdt3\n",
       "S7qvg3OxnVJHREQP9Cw52H5S0pHAWcB04HjbCyQdUfZ/w/bPJO0v6Wbgz8A7hju3V7FGRMSKZPdd\n",
       "rUxERDRswvaQ7rdOcpJOkHS3pGuajmWApJmSzpd0naRrJb2/D2JaTdLFkq6UdL2kY5uOaYCk6ZKu\n",
       "kPSTpmMZIGmRpKtLXJc0HQ+ApPUknS5pQfkb7tVwPM8tv5+B5aE++bf+sfJ/7xpJp0p6Rh/E9IES\n",
       "z7WSPjDssROx5FA6yd0A7EvVqH0pcGiTVU+SXgY8Apxs+/lNxVFX+oxsbPtKSWsBlwF/03QVnaQ1\n",
       "bD8qaQbwG+BDtn/TZEwlrn8BXgisbfugpuMBkHQr8ELb9zcdywBJJwG/sn1C+RuuafuhpuMCkDSN\n",
       "6jNhT9u3j3R8D+PYEvgl8Dzbj0v6PvAz2yc1GNPOwPeAPYClwC+Af7B9S7vjJ2rJoe86ydm+AHig\n",
       "yRgGs/1H21eW148AC6j6kDTK9qPl5apUbUqNf/BJ2gzYH/g2/fd4dd/EI2ld4GW2T4CqfbBfEkOx\n",
       "L3BLk4mheJjqA3iNkkDXoEpaTdoBuNj2Y7afAn4FvG6ogydqcuikg13UlG8ys4CLm42k+nYn6Urg\n",
       "buB829cx8oe6AAAGa0lEQVQ3HRPwJeDDwLKmAxnEwLmS5kt6d9PBAFsB90r6jqTLJX1L0hpNB1Vz\n",
       "CHBq00GUkt4Xgduonrh80Pa5zUbFtcDLJD2z/M0OoOo+0NZETQ4Try6sQaVK6XTgA6UE0Sjby2zv\n",
       "RvUP8+VNjz0j6a+Be2xfQR99Sy/2tj0L2A94b6m+bNIM4AXA12y/gOopw482G1JF0qrAgcB/90Es\n",
       "2wD/BGxJVVpfS9JhTcZkeyHwOeBs4OfAFQzzZWiiJodOOtgFIGkV4AfAd23/qOl46kp1xP8Auzcc\n",
       "ykuAg0r9/veAV0o6ueGYALB9V/l5L/BDqirVJt0B3GH70rJ+OlWy6Af7AZeV31XTdgcutH2f7SeB\n",
       "M6j+nTXK9gm2d7f9CuBBqrbbtiZqcni6g135tvBGqg51USNJwPHA9ba/3HQ8AJI2lLReeb068FdU\n",
       "32AaY/t/2Z5peyuqaolf2n5rkzFB1XAvae3yek3g1UCjT8PZ/iNwu6Tty6Z9gesaDKnuUKrk3g8W\n",
       "AntJWr38P9wXaLz6VNKzys/NgdcyTBVcL3tI90w/dpKT9D3gFcAGkm4HPmn7O03GBOwNvBm4WtLA\n",
       "B/DHbP+iwZieA5xUniqZBpxi+7wG42mnX6otnw38sPpsYQYw1/bZzYYEwPuAueWL2S2UzqtNKslz\n",
       "X6Af2mWwfVUpfc6nqrq5HPhms1EBcLqkDagay99j++GhDpyQj7JGRERvTdRqpYiI6KEkh4iIaJHk\n",
       "EBERLZIcIiKiRZJDRES0SHKIiIgWSQ7RVyQtk/SF2vqHujXFoqQTJb2+G9ca4T5vKENZnzdo+5YD\n",
       "Q7pL2lXSfl2857qS/rG2vomkxoeRiIkrySH6zRPAa0tHHehuh7SVvlYZWbNThwPvsv2qYY6ZRTUK\n",
       "bLdiWB94z8CK7Tttv2E014+oS3KIfrOUqifpPw/eMfibv6RHys/Zkn4l6UeSbpH0WUlvkXRJmShn\n",
       "69pl9pV0qaQbJB1Qzp8u6fPl+Ksk/X3tuhdI+jFthoiQdGi5/jWSPlu2fZKqZ/oJkv5PuzdYxrv6\n",
       "FPDGMjnNGyStqWrCqIvLaKcHlWPfLunMUgo5pxx3rqTLyr0H5p34LLBNud7nJG0h6dpyjdXKKKpX\n",
       "l2vPrl37DEk/l3SjpM/Vfh8nlvd1taR/6uxPF5PJhBw+Iya9r1EN+TH4w3XwN//6+i5U49U/ANwK\n",
       "fMv2nqpmBHsfVbIRsIXtPSRtC5xffr6NakjlPVXN1vUbSQPDVMwCdrL9h/qNJW1C9YH8AqoBzM6W\n",
       "dLDtT0naB/ig7cvbvTnbSyV9gmoSn/eX6/07cJ7td5axpy6WNDDE8yzg+bYfVDXR1WttL5G0IXAR\n",
       "1bhiR5U4Z5XrbVn7/bwXeMr2LpKeW2IdGBtpV2A3qhLbDZK+SjVsxyYDk1apmsMhppiUHKLv2F4C\n",
       "nAyMZqrHS23fbfsJ4GaqcbegGsN+y4FLA/9V7nEz8HuqhPJq4K1l/KnfAc8Eti3nXDI4MRR7UM1F\n",
       "cV+ZOGUu8PLa/pGG/tagY14NfLTEcD7wDGDzEvM5th8sx00DjpV0FXAOsEkZTG24++0NfLe87xuA\n",
       "PwDbl2ufZ3uJ7cepBobbnGq8pK0lfUXSa6gmrokpJiWH6FdfphqsrD544ZOULzRl4L5Va/ser71e\n",
       "VltfxvD/zge+XR9p+5z6jlL98udhzqt/IIsVSzIr077xOts3DYrhRYNiOAzYEHiB7adUDTO+WgfX\n",
       "Hip51H9vTwEzSgllV+A1wD8Af0fVjhJTSEoO0ZdsP0D1Lf9wln/QLqKa4xngIGCVUV5WwBtU2QbY\n",
       "mmpo5bOA9ww0+EraXiPPbnYp8ApJG5SqnkOopl3s1MPA2rX1s6iVlCTNqsVctw7VxERPleqrLcr2\n",
       "JYOuV3cBVVKhVCdtTvW+2yUMlYcBpts+A/gE/TNfQ4yjJIfoN/Vv3F+k+pY84FtUH8hXAnsBjwxx\n",
       "3uDrufb6NuAS4GfAEaUa6ttUVSqXl0dNv05V2qifu+JFq0l4PkpVBXQlMN/2T0bx/s4HdhxokAY+\n",
       "DaxSGoCvBf61TfxQVV/tLulq4C1U84Jj+z7gt6UR+XODzvsaMK2ccxrwtjL3erv3Z6opd88vVVyn\n",
       "0CczvcX4ypDdERHRIiWHiIhokeQQEREtkhwiIqJFkkNERLRIcoiIiBZJDhER0SLJISIiWiQ5RERE\n",
       "i/8PyGMkiPqbHTwAAAAASUVORK5CYII=\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a2dda50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue training neural network ([y]/n)? n\n"
     ]
    }
   ],
   "source": [
    "# get the training data from file\n",
    "with gzip.open('../Data/zip.train.gz') as f:\n",
    "    train = loadtxt(f)\n",
    "    \n",
    "# separate the observations from the labels and vectorize the labels\n",
    "X_train = train[:,1:]\n",
    "y_train, num_labels = vectorize_digit_labels(train[:,0][:,newaxis])\n",
    "\n",
    "# reduce the dimensions of the data to 20\n",
    "X_train = pca(X_train, 16)\n",
    "\n",
    "# train weights using single hidden layer neural network\n",
    "W_1, W_2 = single_hidden_train(X_train, y_train, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
